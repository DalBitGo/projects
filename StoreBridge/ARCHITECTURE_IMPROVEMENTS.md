# ì•„í‚¤í…ì²˜ ê°œì„ ì‚¬í•­ ìƒì„¸ ì„¤ê³„

> ARCHITECTURE.md ë¦¬ë·° ê²°ê³¼ ë°œê²¬ëœ ë¬¸ì œì  í•´ê²° ë°©ì•ˆ

**ì‘ì„±ì¼**: 2025-10-16
**ë²„ì „**: 1.1

---

## ëª©ì°¨

1. [P0: Race Condition in Rate Limiter](#p0-race-condition-in-rate-limiter)
2. [P1: ì˜µì…˜ ë§¤í•‘ ë¡œì§ ê°•í™”](#p1-ì˜µì…˜-ë§¤í•‘-ë¡œì§-ê°•í™”)
3. [P1: ì´ë¯¸ì§€ ë³‘ë ¬ ì²˜ë¦¬ ì „ëµ](#p1-ì´ë¯¸ì§€-ë³‘ë ¬-ì²˜ë¦¬-ì „ëµ)
4. [P2: ìºì‹œ ë¬´íš¨í™” ì „ëµ](#p2-ìºì‹œ-ë¬´íš¨í™”-ì „ëµ)
5. [P2: ê°€ê²© ë™ê¸°í™” Delta ê°ì§€](#p2-ê°€ê²©-ë™ê¸°í™”-delta-ê°ì§€)
6. [P3: Prometheus ë©”íŠ¸ë¦­ ìƒì„¸í™”](#p3-prometheus-ë©”íŠ¸ë¦­-ìƒì„¸í™”)

---

## P0: Race Condition in Rate Limiter

### ë¬¸ì œì 

```python
# í˜„ì¬ ì½”ë“œ (ARCHITECTURE.md)
current_count = await self.redis.get(key)
current_count = int(current_count) if current_count else 0

if current_count < self.max_tps:
    await self.redis.incr(key)  # ğŸš¨ Race Condition!
    return True
```

**ì‹œë‚˜ë¦¬ì˜¤:**
```
ì‹œê°„ T:
  Worker A: GET â†’ count=1
  Worker B: GET â†’ count=1
  Worker A: CHECK (1 < 2) â†’ OK
  Worker B: CHECK (1 < 2) â†’ OK
  Worker A: INCR â†’ count=2
  Worker B: INCR â†’ count=3  # ğŸš¨ ì´ˆê³¼!
```

### í•´ê²°ì±…: Lua Scriptë¡œ ì›ìì„± ë³´ì¥

#### ê°œì„ ëœ ì½”ë“œ

```python
# app/connectors/rate_limiters.py

import time
import asyncio
from typing import Optional
import aioredis
from aioredis.client import Redis

class TokenBucketRateLimiter:
    """
    ë„¤ì´ë²„ ì»¤ë¨¸ìŠ¤ API Rate Limit ì¤€ìˆ˜
    - TPS: 2
    - Burst Max: ë‹¤ìŒ 1ì´ˆ ì„ ë¹Œë¦¼ (ì—°ì† ë¶ˆê°€)
    - ì›ìì„±: Lua Script
    """

    # Lua Script: ì›ìì ìœ¼ë¡œ ì²´í¬ & ì¦ê°€
    LUA_ACQUIRE = """
    local key = KEYS[1]
    local max_tps = tonumber(ARGV[1])
    local ttl = tonumber(ARGV[2])

    local current = redis.call('GET', key)

    if not current then
        current = 0
    else
        current = tonumber(current)
    end

    if current < max_tps then
        redis.call('INCR', key)
        redis.call('EXPIRE', key, ttl)
        return 1  -- ì„±ê³µ
    else
        return 0  -- ì‹¤íŒ¨
    end
    """

    # Burst Maxìš© Lua Script
    LUA_ACQUIRE_BURST = """
    local current_key = KEYS[1]
    local next_key = KEYS[2]
    local burst_flag_key = KEYS[3]
    local max_tps = tonumber(ARGV[1])
    local ttl = tonumber(ARGV[2])

    -- í˜„ì¬ ì´ˆ ì²´í¬
    local current = redis.call('GET', current_key)
    if not current then
        current = 0
    else
        current = tonumber(current)
    end

    if current < max_tps then
        redis.call('INCR', current_key)
        redis.call('EXPIRE', current_key, ttl)
        return 1  -- ì¼ë°˜ ì„±ê³µ
    end

    -- Burst Max ì²´í¬
    local burst_used = redis.call('GET', burst_flag_key)
    if burst_used then
        return 0  -- Burst ì´ë¯¸ ì‚¬ìš©ë¨
    end

    local next = redis.call('GET', next_key)
    if not next then
        next = 0
    else
        next = tonumber(next)
    end

    if next == 0 then
        redis.call('INCR', next_key)
        redis.call('EXPIRE', next_key, ttl)
        redis.call('SETEX', burst_flag_key, ttl, '1')
        return 2  -- Burst ì„±ê³µ
    end

    return 0  -- ì‹¤íŒ¨
    """

    def __init__(
        self,
        redis_url: str,
        max_tps: int = 2,
        burst_enabled: bool = True
    ):
        self.redis: Optional[Redis] = None
        self.redis_url = redis_url
        self.max_tps = max_tps
        self.burst_enabled = burst_enabled
        self.ttl = 2  # í‚¤ TTL (ì´ˆ)

    async def connect(self):
        """Redis ì—°ê²° ì´ˆê¸°í™”"""
        if not self.redis:
            self.redis = await aioredis.from_url(self.redis_url)

    async def acquire(self, timeout: Optional[float] = None) -> bool:
        """
        í† í° íšë“ ì‹œë„

        Args:
            timeout: ìµœëŒ€ ëŒ€ê¸° ì‹œê°„ (ì´ˆ). Noneì´ë©´ ì¦‰ì‹œ ì‹¤íŒ¨.

        Returns:
            True: í† í° íšë“ ì„±ê³µ
            False: timeout ë‚´ íšë“ ì‹¤íŒ¨

        Raises:
            RateLimitExceeded: timeout ì—†ì´ ì¦‰ì‹œ ì‹¤íŒ¨
        """
        await self.connect()
        start_time = time.time()

        while True:
            now = time.time()
            current_second = int(now)

            # 1ì°¨ ì‹œë„: ì¼ë°˜ íšë“
            result = await self._try_acquire_normal(current_second)
            if result:
                return True

            # 2ì°¨ ì‹œë„: Burst Max
            if self.burst_enabled:
                result = await self._try_acquire_burst(current_second)
                if result:
                    return True

            # ì‹¤íŒ¨ ì²˜ë¦¬
            if timeout is None:
                raise RateLimitExceeded(
                    f"Rate limit exceeded: {self.max_tps} TPS"
                )

            elapsed = time.time() - start_time
            if elapsed >= timeout:
                return False

            # ë‹¤ìŒ ì´ˆê¹Œì§€ ëŒ€ê¸°
            wait_time = 1 - (now - current_second)
            await asyncio.sleep(max(wait_time, 0.1))

    async def _try_acquire_normal(self, current_second: int) -> bool:
        """ì¼ë°˜ í† í° íšë“ ì‹œë„"""
        key = f'naver:ratelimit:{current_second}'

        result = await self.redis.eval(
            self.LUA_ACQUIRE,
            1,  # key ê°œìˆ˜
            key,
            self.max_tps,
            self.ttl
        )

        return result == 1

    async def _try_acquire_burst(self, current_second: int) -> bool:
        """Burst Max í† í° íšë“ ì‹œë„"""
        current_key = f'naver:ratelimit:{current_second}'
        next_key = f'naver:ratelimit:{current_second + 1}'
        burst_flag_key = f'naver:burst_used:{current_second}'

        result = await self.redis.eval(
            self.LUA_ACQUIRE_BURST,
            3,  # key ê°œìˆ˜
            current_key,
            next_key,
            burst_flag_key,
            self.max_tps,
            self.ttl
        )

        return result > 0

    async def get_remaining(self) -> int:
        """í˜„ì¬ ì´ˆì˜ ë‚¨ì€ í† í° ìˆ˜ (ë””ë²„ê¹…ìš©)"""
        await self.connect()
        current_second = int(time.time())
        key = f'naver:ratelimit:{current_second}'

        current = await self.redis.get(key)
        current = int(current) if current else 0

        return max(0, self.max_tps - current)

    async def close(self):
        """Redis ì—°ê²° ì¢…ë£Œ"""
        if self.redis:
            await self.redis.close()


class RateLimitExceeded(Exception):
    """Rate Limit ì´ˆê³¼ ì˜ˆì™¸"""
    pass
```

#### í…ŒìŠ¤íŠ¸ ì½”ë“œ

```python
# tests/test_rate_limiter.py

import pytest
import asyncio
from app.connectors.rate_limiters import TokenBucketRateLimiter

@pytest.mark.asyncio
async def test_rate_limiter_allows_max_tps():
    """ìµœëŒ€ TPSê¹Œì§€ í—ˆìš©"""
    limiter = TokenBucketRateLimiter(
        redis_url='redis://localhost:6379',
        max_tps=2
    )

    # ì²« 2ê°œ ì„±ê³µ
    assert await limiter.acquire() == True
    assert await limiter.acquire() == True

    # 3ë²ˆì§¸ ì‹¤íŒ¨
    with pytest.raises(Exception):  # RateLimitExceeded
        await limiter.acquire(timeout=None)

    await limiter.close()


@pytest.mark.asyncio
async def test_rate_limiter_resets_per_second():
    """ë§¤ ì´ˆë§ˆë‹¤ ë¦¬ì…‹"""
    limiter = TokenBucketRateLimiter(
        redis_url='redis://localhost:6379',
        max_tps=2
    )

    # 2ê°œ ì†Œì§„
    await limiter.acquire()
    await limiter.acquire()

    # 1ì´ˆ ëŒ€ê¸°
    await asyncio.sleep(1)

    # ë‹¤ì‹œ 2ê°œ ê°€ëŠ¥
    assert await limiter.acquire() == True
    assert await limiter.acquire() == True

    await limiter.close()


@pytest.mark.asyncio
async def test_rate_limiter_concurrent_workers():
    """ì—¬ëŸ¬ ì›Œì»¤ê°€ ë™ì‹œ í˜¸ì¶œ ì‹œ Race Condition ì—†ìŒ"""
    limiter = TokenBucketRateLimiter(
        redis_url='redis://localhost:6379',
        max_tps=10
    )

    success_count = 0
    fail_count = 0

    async def worker():
        nonlocal success_count, fail_count
        try:
            if await limiter.acquire(timeout=None):
                success_count += 1
        except Exception:
            fail_count += 1

    # 20ê°œ ì›Œì»¤ ë™ì‹œ ì‹¤í–‰
    await asyncio.gather(*[worker() for _ in range(20)])

    # ì •í™•íˆ 10ê°œë§Œ ì„±ê³µ
    assert success_count == 10
    assert fail_count == 10

    await limiter.close()
```

---

## P1: ì˜µì…˜ ë§¤í•‘ ë¡œì§ ê°•í™”

### ë¬¸ì œì 

ë„ë§¤ê¾¹ê³¼ ë„¤ì´ë²„ì˜ ì˜µì…˜ êµ¬ì¡°ê°€ ë‹¤ë¦„:

```
ë„ë§¤ê¾¹:
  ["ë¸”ë™-S", "ë¸”ë™-M", "í™”ì´íŠ¸-S", "í™”ì´íŠ¸-M"]
  â†’ ë‹¨ìˆœ ë¬¸ìì—´ ë°°ì—´

ë„¤ì´ë²„:
  {
    "type": "COMBINATION",
    "dimensions": [
      {"name": "ìƒ‰ìƒ", "values": ["ë¸”ë™", "í™”ì´íŠ¸"]},
      {"name": "ì‚¬ì´ì¦ˆ", "values": ["S", "M"]}
    ],
    "combinations": [
      {"values": ["ë¸”ë™", "S"], "price": 0, "stock": 10},
      ...
    ]
  }
  â†’ êµ¬ì¡°í™”ëœ ê°ì²´
```

### í•´ê²°ì±…: ì˜µì…˜ íŒŒì„œ & ë§¤í¼

#### ì˜µì…˜ íƒ€ì… ë¶„ë¥˜

```python
from enum import Enum

class OptionType(str, Enum):
    """ë„¤ì´ë²„ ì˜µì…˜ íƒ€ì…"""
    NONE = "NONE"              # ì˜µì…˜ ì—†ìŒ
    SIMPLE = "SIMPLE"          # ë‹¨ì¼ ì˜µì…˜ (ìƒ‰ìƒë§Œ)
    COMBINATION = "COMBINATION"  # ì¡°í•© ì˜µì…˜ (ìƒ‰ìƒ Ã— ì‚¬ì´ì¦ˆ)
    INDEPENDENT = "INDEPENDENT"  # ë…ë¦½ ì˜µì…˜ (ì¶”ê°€ êµ¬ì„±í’ˆ)
```

#### ì˜µì…˜ íŒŒì„œ

```python
# app/transformers/option_mapper.py

import re
from typing import List, Dict, Any, Optional, Tuple
from itertools import product

class OptionMapper:
    """ë„ë§¤ê¾¹ â†’ ë„¤ì´ë²„ ì˜µì…˜ ë³€í™˜"""

    # ì˜µì…˜ëª… ì •ê·œí™” ë§¤í•‘
    OPTION_NAME_MAP = {
        'ìƒ‰ìƒ': ['ìƒ‰ìƒ', 'ìƒ‰ê¹”', 'ì»¬ëŸ¬', 'color', 'COLOR'],
        'ì‚¬ì´ì¦ˆ': ['ì‚¬ì´ì¦ˆ', 'í¬ê¸°', 'ì¹˜ìˆ˜', 'size', 'SIZE', 'Size'],
        'ê¸¸ì´': ['ê¸¸ì´', 'length', 'LENGTH'],
        'ë‘ê»˜': ['ë‘ê»˜', 'thickness'],
    }

    # êµ¬ë¶„ì í›„ë³´
    SEPARATORS = ['-', '/', '_', ' ', ':']

    def parse(self, raw_options: List[str]) -> Dict[str, Any]:
        """
        ë„ë§¤ê¾¹ ì˜µì…˜ì„ ë„¤ì´ë²„ ì˜µì…˜ êµ¬ì¡°ë¡œ ë³€í™˜

        Args:
            raw_options: ["ë¸”ë™-S", "ë¸”ë™-M", "í™”ì´íŠ¸-S", "í™”ì´íŠ¸-M"]

        Returns:
            {
                "type": "COMBINATION",
                "dimensions": [...],
                "combinations": [...]
            }
        """
        if not raw_options or len(raw_options) == 0:
            return {"type": "NONE"}

        # 1. êµ¬ë¶„ì íƒì§€
        separator = self._detect_separator(raw_options)

        if separator is None:
            # êµ¬ë¶„ì ì—†ìŒ â†’ ë‹¨ì¼ ì˜µì…˜
            return self._parse_simple(raw_options)

        # 2. ê°’ ë¶„ë¦¬
        split_options = [opt.split(separator) for opt in raw_options]

        # 3. ì°¨ì› ìˆ˜ í™•ì¸
        dimensions_count = len(split_options[0])

        if dimensions_count == 1:
            return self._parse_simple(raw_options)
        else:
            return self._parse_combination(split_options)

    def _detect_separator(self, options: List[str]) -> Optional[str]:
        """êµ¬ë¶„ì íƒì§€"""
        for sep in self.SEPARATORS:
            # ëª¨ë“  ì˜µì…˜ì— í•´ë‹¹ êµ¬ë¶„ìê°€ ìˆê³ , ì¼ê´€ëœ ê°œìˆ˜ì¸ì§€ í™•ì¸
            split_counts = [opt.count(sep) for opt in options]

            if all(count > 0 for count in split_counts) and \
               len(set(split_counts)) == 1:
                return sep

        return None

    def _parse_simple(self, options: List[str]) -> Dict[str, Any]:
        """ë‹¨ì¼ ì˜µì…˜ íŒŒì‹±"""
        # ì˜µì…˜ëª… ì¶”ë¡  (ëŒ€ë¶€ë¶„ "ìƒ‰ìƒ")
        option_name = self._infer_option_name(options)

        return {
            "type": "SIMPLE",
            "dimensions": [
                {
                    "name": option_name,
                    "values": options
                }
            ]
        }

    def _parse_combination(self, split_options: List[List[str]]) -> Dict[str, Any]:
        """ì¡°í•© ì˜µì…˜ íŒŒì‹±"""
        # 1. ê° ì°¨ì›ë³„ ê³ ìœ ê°’ ì¶”ì¶œ
        dimensions_count = len(split_options[0])
        dimension_values = [set() for _ in range(dimensions_count)]

        for option in split_options:
            for i, value in enumerate(option):
                dimension_values[i].add(value.strip())

        # 2. ì°¨ì›ë³„ ì´ë¦„ ì¶”ë¡ 
        dimensions = []
        for i, values in enumerate(dimension_values):
            name = self._infer_dimension_name(i, values)
            dimensions.append({
                "name": name,
                "values": sorted(list(values))
            })

        # 3. ì¡°í•© ìƒì„± (ì¹´í‹°ì…˜ í”„ë¡œë•íŠ¸)
        combinations = []
        all_combinations = product(*[d['values'] for d in dimensions])

        for combo in all_combinations:
            combinations.append({
                "values": list(combo),
                "price": 0,  # ê¸°ë³¸ê°’ (ì¶”í›„ ê°€ê²© ì°¨ì´ ë°˜ì˜)
                "stock": 999  # ê¸°ë³¸ê°’ (ì¶”í›„ ì‹¤ì œ ì¬ê³  ë°˜ì˜)
            })

        return {
            "type": "COMBINATION",
            "dimensions": dimensions,
            "combinations": combinations
        }

    def _infer_option_name(self, values: List[str]) -> str:
        """ì˜µì…˜ëª… ì¶”ë¡  (ë‹¨ì¼ ì˜µì…˜)"""
        # ìƒ‰ìƒ ê´€ë ¨ í‚¤ì›Œë“œ ì²´í¬
        color_keywords = ['ë¸”ë™', 'í™”ì´íŠ¸', 'ë ˆë“œ', 'ë¸”ë£¨', 'ê·¸ë ˆì´', 'ë„¤ì´ë¹„']
        if any(keyword in ''.join(values) for keyword in color_keywords):
            return 'ìƒ‰ìƒ'

        # ì‚¬ì´ì¦ˆ ê´€ë ¨ í‚¤ì›Œë“œ ì²´í¬
        size_keywords = ['S', 'M', 'L', 'XL', 'FREE']
        if any(keyword in ''.join(values).upper() for keyword in size_keywords):
            return 'ì‚¬ì´ì¦ˆ'

        # ê¸°ë³¸ê°’
        return 'ì˜µì…˜'

    def _infer_dimension_name(self, index: int, values: set) -> str:
        """ì°¨ì›ë³„ ì˜µì…˜ëª… ì¶”ë¡  (ì¡°í•© ì˜µì…˜)"""
        values_str = ''.join(values).upper()

        # ìƒ‰ìƒ ì²´í¬
        color_keywords = ['ë¸”ë™', 'í™”ì´íŠ¸', 'ë ˆë“œ', 'ë¸”ë£¨', 'BLACK', 'WHITE']
        if any(keyword.upper() in values_str for keyword in color_keywords):
            return 'ìƒ‰ìƒ'

        # ì‚¬ì´ì¦ˆ ì²´í¬
        size_keywords = ['S', 'M', 'L', 'XL', 'FREE']
        if any(keyword in values_str for keyword in size_keywords):
            return 'ì‚¬ì´ì¦ˆ'

        # ìˆ«ìë§Œ ìˆìœ¼ë©´ "ê¸¸ì´" or "ë‘ê»˜"
        if all(v.isdigit() for v in values):
            return 'ê¸¸ì´' if index == 1 else 'ë‘ê»˜'

        # ê¸°ë³¸ê°’
        return f'ì˜µì…˜{index + 1}'

    def normalize_option_name(self, name: str) -> str:
        """ì˜µì…˜ëª… ì •ê·œí™”"""
        for standard, aliases in self.OPTION_NAME_MAP.items():
            if name in aliases:
                return standard
        return name
```

#### í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤

```python
# tests/test_option_mapper.py

import pytest
from app.transformers.option_mapper import OptionMapper

def test_parse_no_options():
    """ì˜µì…˜ ì—†ëŠ” ìƒí’ˆ"""
    mapper = OptionMapper()
    result = mapper.parse([])
    assert result['type'] == 'NONE'


def test_parse_simple_option():
    """ë‹¨ì¼ ì˜µì…˜ (ìƒ‰ìƒë§Œ)"""
    mapper = OptionMapper()
    result = mapper.parse(['ë¸”ë™', 'í™”ì´íŠ¸', 'ê·¸ë ˆì´'])

    assert result['type'] == 'SIMPLE'
    assert result['dimensions'][0]['name'] == 'ìƒ‰ìƒ'
    assert set(result['dimensions'][0]['values']) == {'ë¸”ë™', 'í™”ì´íŠ¸', 'ê·¸ë ˆì´'}


def test_parse_combination_option():
    """ì¡°í•© ì˜µì…˜ (ìƒ‰ìƒ Ã— ì‚¬ì´ì¦ˆ)"""
    mapper = OptionMapper()
    result = mapper.parse(['ë¸”ë™-S', 'ë¸”ë™-M', 'í™”ì´íŠ¸-S', 'í™”ì´íŠ¸-M'])

    assert result['type'] == 'COMBINATION'
    assert len(result['dimensions']) == 2

    # ì°¨ì› í™•ì¸
    assert result['dimensions'][0]['name'] == 'ìƒ‰ìƒ'
    assert set(result['dimensions'][0]['values']) == {'ë¸”ë™', 'í™”ì´íŠ¸'}

    assert result['dimensions'][1]['name'] == 'ì‚¬ì´ì¦ˆ'
    assert set(result['dimensions'][1]['values']) == {'S', 'M'}

    # ì¡°í•© í™•ì¸ (2Ã—2 = 4ê°œ)
    assert len(result['combinations']) == 4


def test_parse_three_dimension_combination():
    """3ì°¨ì› ì¡°í•© (ìƒ‰ìƒ Ã— ì‚¬ì´ì¦ˆ Ã— ê¸¸ì´)"""
    mapper = OptionMapper()
    result = mapper.parse([
        'ë¸”ë™-S-90', 'ë¸”ë™-S-100',
        'ë¸”ë™-M-90', 'ë¸”ë™-M-100',
        'í™”ì´íŠ¸-S-90', 'í™”ì´íŠ¸-S-100',
        'í™”ì´íŠ¸-M-90', 'í™”ì´íŠ¸-M-100'
    ])

    assert result['type'] == 'COMBINATION'
    assert len(result['dimensions']) == 3
    assert len(result['combinations']) == 8  # 2Ã—2Ã—2


def test_detect_separator():
    """ë‹¤ì–‘í•œ êµ¬ë¶„ì íƒì§€"""
    mapper = OptionMapper()

    # í•˜ì´í”ˆ
    assert mapper._detect_separator(['ë¸”ë™-S', 'í™”ì´íŠ¸-M']) == '-'

    # ìŠ¬ë˜ì‹œ
    assert mapper._detect_separator(['ë¸”ë™/S', 'í™”ì´íŠ¸/M']) == '/'

    # ì–¸ë”ìŠ¤ì½”ì–´
    assert mapper._detect_separator(['ë¸”ë™_S', 'í™”ì´íŠ¸_M']) == '_'

    # êµ¬ë¶„ì ì—†ìŒ
    assert mapper._detect_separator(['ë¸”ë™', 'í™”ì´íŠ¸']) is None
```

---

## P1: ì´ë¯¸ì§€ ë³‘ë ¬ ì²˜ë¦¬ ì „ëµ

### ë¬¸ì œì 

ìƒí’ˆë‹¹ 10~20ì¥ ì´ë¯¸ì§€ë¥¼ ìˆœì°¨ ì²˜ë¦¬ ì‹œ ë³‘ëª©:

```python
# ìˆœì°¨ ì²˜ë¦¬ (ëŠë¦¼)
for img_url in product.images:
    data = await download(img_url)        # I/O
    resized = resize(data)                # CPU
    await upload_to_s3(resized)           # I/O
    await upload_to_naver(resized)        # I/O + Rate Limit
```

### í•´ê²°ì±…: ë‹¨ê³„ë³„ ë³‘ë ¬ ì²˜ë¦¬

```python
# app/services/image_pipeline.py

import asyncio
import hashlib
from typing import List, Dict, Any, Optional
from concurrent.futures import ThreadPoolExecutor
from io import BytesIO
from PIL import Image
import httpx

class ImagePipeline:
    """ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ, ë³€í™˜, ì—…ë¡œë“œ íŒŒì´í”„ë¼ì¸"""

    def __init__(
        self,
        s3_client,
        naver_client,
        max_images: int = 10,
        min_width: int = 500,
        min_height: int = 500,
        max_size_mb: int = 12
    ):
        self.s3_client = s3_client
        self.naver_client = naver_client
        self.max_images = max_images
        self.min_width = min_width
        self.min_height = min_height
        self.max_size_mb = max_size_mb
        self.executor = ThreadPoolExecutor(max_workers=4)

    async def process(
        self,
        image_urls: List[str],
        product_id: str
    ) -> List[str]:
        """
        ì´ë¯¸ì§€ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

        Args:
            image_urls: ì›ë³¸ ì´ë¯¸ì§€ URL ëª©ë¡
            product_id: ìƒí’ˆ ID (S3 ê²½ë¡œìš©)

        Returns:
            ë„¤ì´ë²„ ì´ë¯¸ì§€ URL ëª©ë¡
        """
        # 1ë‹¨ê³„: ë³‘ë ¬ ë‹¤ìš´ë¡œë“œ (I/O bound)
        print(f"[Image Pipeline] 1/4 ë³‘ë ¬ ë‹¤ìš´ë¡œë“œ ì¤‘... ({len(image_urls[:self.max_images])}ì¥)")
        images_data = await self._download_images(image_urls[:self.max_images])

        # 2ë‹¨ê³„: ë³‘ë ¬ ë³€í™˜ (CPU bound)
        print(f"[Image Pipeline] 2/4 ë³‘ë ¬ ë³€í™˜ ì¤‘... ({len(images_data)}ì¥)")
        processed_images = await self._process_images(images_data)

        # 3ë‹¨ê³„: ì¤‘ë³µ ì œê±° (í•´ì‹œ ê¸°ë°˜)
        print(f"[Image Pipeline] 3/4 ì¤‘ë³µ ì œê±° ì¤‘...")
        unique_images = self._deduplicate_images(processed_images)
        print(f"[Image Pipeline] ì¤‘ë³µ ì œê±° ì™„ë£Œ: {len(processed_images)} â†’ {len(unique_images)}ì¥")

        # 4ë‹¨ê³„: S3 ì—…ë¡œë“œ (ë°±ì—…ìš©, ë³‘ë ¬)
        print(f"[Image Pipeline] 4/4 S3 ì—…ë¡œë“œ ì¤‘...")
        s3_urls = await self._upload_to_s3(unique_images, product_id)

        # 5ë‹¨ê³„: ë„¤ì´ë²„ ì—…ë¡œë“œëŠ” Rate Limiter í†µê³¼í•´ì•¼ í•˜ë¯€ë¡œ ìˆœì°¨
        # (ë„¤ì´ë²„ APIì—ì„œ S3 URL ì§ì ‘ ë“±ë¡ ê°€ëŠ¥í•˜ë©´ ì´ ë‹¨ê³„ ìƒëµ)
        print(f"[Image Pipeline] ì™„ë£Œ: {len(s3_urls)}ì¥")
        return s3_urls

    async def _download_images(self, urls: List[str]) -> List[bytes]:
        """ë³‘ë ¬ ë‹¤ìš´ë¡œë“œ"""
        async def download_one(url: str) -> Optional[bytes]:
            try:
                async with httpx.AsyncClient(timeout=10.0) as client:
                    response = await client.get(url)
                    response.raise_for_status()
                    return response.content
            except Exception as e:
                print(f"[Image Pipeline] ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {url} - {e}")
                return None

        results = await asyncio.gather(*[download_one(url) for url in urls])
        return [data for data in results if data is not None]

    async def _process_images(self, images_data: List[bytes]) -> List[bytes]:
        """ë³‘ë ¬ ì´ë¯¸ì§€ ë³€í™˜ (CPU bound â†’ ThreadPoolExecutor)"""
        loop = asyncio.get_event_loop()

        tasks = [
            loop.run_in_executor(self.executor, self._process_one_image, data)
            for data in images_data
        ]

        results = await asyncio.gather(*tasks)
        return [img for img in results if img is not None]

    def _process_one_image(self, data: bytes) -> Optional[bytes]:
        """ë‹¨ì¼ ì´ë¯¸ì§€ ë³€í™˜ (ë™ê¸° í•¨ìˆ˜)"""
        try:
            img = Image.open(BytesIO(data))

            # 1. ê·œê²© ê²€ì¦
            if img.width < self.min_width or img.height < self.min_height:
                print(f"[Image Pipeline] ê·œê²© ë¯¸ë‹¬: {img.width}x{img.height}")
                return None

            # 2. ë¦¬ì‚¬ì´ì¦ˆ (ìµœëŒ€ 1200px, ë¹„ìœ¨ ìœ ì§€)
            max_dimension = 1200
            if img.width > max_dimension or img.height > max_dimension:
                ratio = min(max_dimension / img.width, max_dimension / img.height)
                new_size = (int(img.width * ratio), int(img.height * ratio))
                img = img.resize(new_size, Image.Resampling.LANCZOS)

            # 3. WebP ë³€í™˜ (ìš©ëŸ‰ ì ˆê°)
            output = BytesIO()
            img.save(output, format='WEBP', quality=85)
            output.seek(0)

            result = output.read()

            # 4. ìš©ëŸ‰ ì²´í¬
            size_mb = len(result) / (1024 * 1024)
            if size_mb > self.max_size_mb:
                print(f"[Image Pipeline] ìš©ëŸ‰ ì´ˆê³¼: {size_mb:.2f}MB")
                return None

            return result

        except Exception as e:
            print(f"[Image Pipeline] ë³€í™˜ ì‹¤íŒ¨: {e}")
            return None

    def _deduplicate_images(self, images: List[bytes]) -> List[bytes]:
        """ì¤‘ë³µ ì´ë¯¸ì§€ ì œê±° (í•´ì‹œ ê¸°ë°˜)"""
        seen_hashes = set()
        unique_images = []

        for img_data in images:
            img_hash = hashlib.md5(img_data).hexdigest()

            if img_hash not in seen_hashes:
                seen_hashes.add(img_hash)
                unique_images.append(img_data)

        return unique_images

    async def _upload_to_s3(
        self,
        images: List[bytes],
        product_id: str
    ) -> List[str]:
        """S3 ë³‘ë ¬ ì—…ë¡œë“œ"""
        async def upload_one(img_data: bytes, index: int) -> Optional[str]:
            try:
                filename = f"{product_id}_{index}.webp"
                url = await self.s3_client.upload(
                    data=img_data,
                    filename=filename,
                    content_type='image/webp'
                )
                return url
            except Exception as e:
                print(f"[Image Pipeline] S3 ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
                return None

        results = await asyncio.gather(*[
            upload_one(img, i) for i, img in enumerate(images)
        ])

        return [url for url in results if url is not None]
```

#### ì„±ëŠ¥ ë¹„êµ

```python
# ë²¤ì¹˜ë§ˆí¬
import time

# ìˆœì°¨ ì²˜ë¦¬
start = time.time()
for url in image_urls[:10]:
    data = await download(url)      # í‰ê·  0.5ì´ˆ
    resized = resize(data)          # í‰ê·  0.2ì´ˆ
    await upload_s3(resized)        # í‰ê·  0.3ì´ˆ
# ì´: 10ì´ˆ

# ë³‘ë ¬ ì²˜ë¦¬
start = time.time()
pipeline = ImagePipeline(...)
await pipeline.process(image_urls[:10], product_id)
# ì´: ~2ì´ˆ (5ë°° ë¹ ë¦„!)
```

---

## P2: ìºì‹œ ë¬´íš¨í™” ì „ëµ

### ë¬¸ì œì 

TTLë§Œ ìˆê³  ëª…ì‹œì  ë¬´íš¨í™” ì—†ìŒ:

```python
@cache(ttl=3600)  # 1ì‹œê°„
async def get_item_view(item_id: str):
    ...
```

â†’ ë„ë§¤ê¾¹ì—ì„œ ê°€ê²© ë³€ê²½ ì‹œ 1ì‹œê°„ ë™ì•ˆ êµ¬ ë°ì´í„° ì‚¬ìš©

### í•´ê²°ì±…: Cache Manager

```python
# app/utils/cache.py

from typing import Optional, Callable, Any
import functools
import hashlib
import json
import aioredis

class CacheManager:
    """Redis ìºì‹œ ê´€ë¦¬ì"""

    def __init__(self, redis_url: str, prefix: str = 'cache'):
        self.redis_url = redis_url
        self.prefix = prefix
        self.redis: Optional[aioredis.Redis] = None

    async def connect(self):
        if not self.redis:
            self.redis = await aioredis.from_url(self.redis_url)

    def cache(self, ttl: int = 3600, key_builder: Optional[Callable] = None):
        """ìºì‹œ ë°ì½”ë ˆì´í„°"""
        def decorator(func: Callable):
            @functools.wraps(func)
            async def wrapper(*args, **kwargs):
                await self.connect()

                # ìºì‹œ í‚¤ ìƒì„±
                cache_key = self._build_cache_key(func, args, kwargs, key_builder)

                # ìºì‹œ ì¡°íšŒ
                cached = await self.redis.get(cache_key)
                if cached:
                    return json.loads(cached)

                # ìºì‹œ ë¯¸ìŠ¤: ì‹¤ì œ í•¨ìˆ˜ í˜¸ì¶œ
                result = await func(*args, **kwargs)

                # ìºì‹œ ì €ì¥
                await self.redis.setex(
                    cache_key,
                    ttl,
                    json.dumps(result, ensure_ascii=False)
                )

                return result

            # ë¬´íš¨í™” ë©”ì„œë“œ ì¶”ê°€
            wrapper.invalidate = functools.partial(
                self._invalidate_pattern,
                func.__name__
            )

            return wrapper
        return decorator

    def _build_cache_key(
        self,
        func: Callable,
        args: tuple,
        kwargs: dict,
        key_builder: Optional[Callable]
    ) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        if key_builder:
            key_suffix = key_builder(*args, **kwargs)
        else:
            # ê¸°ë³¸: í•¨ìˆ˜ëª… + ì¸ì í•´ì‹œ
            args_str = json.dumps(
                {'args': args, 'kwargs': kwargs},
                sort_keys=True,
                ensure_ascii=False
            )
            args_hash = hashlib.md5(args_str.encode()).hexdigest()[:8]
            key_suffix = args_hash

        return f'{self.prefix}:{func.__name__}:{key_suffix}'

    async def invalidate(self, key: str):
        """íŠ¹ì • í‚¤ ë¬´íš¨í™”"""
        await self.connect()
        await self.redis.delete(key)

    async def invalidate_pattern(self, pattern: str):
        """íŒ¨í„´ ë§¤ì¹­ í‚¤ ì¼ê´„ ë¬´íš¨í™”"""
        await self.connect()
        full_pattern = f'{self.prefix}:{pattern}'

        cursor = 0
        deleted_count = 0

        while True:
            cursor, keys = await self.redis.scan(
                cursor=cursor,
                match=full_pattern,
                count=100
            )

            if keys:
                await self.redis.delete(*keys)
                deleted_count += len(keys)

            if cursor == 0:
                break

        return deleted_count

    async def invalidate_product(self, item_id: str):
        """íŠ¹ì • ìƒí’ˆ ê´€ë ¨ ìºì‹œ ì „ì²´ ë¬´íš¨í™”"""
        patterns = [
            f'get_item_view:*{item_id}*',
            f'get_item_list:*',  # ëª©ë¡ì—ë„ í¬í•¨ë  ìˆ˜ ìˆìŒ
        ]

        total_deleted = 0
        for pattern in patterns:
            count = await self.invalidate_pattern(pattern)
            total_deleted += count

        return total_deleted

    async def invalidate_category(self, category_id: str):
        """ì¹´í…Œê³ ë¦¬ ê´€ë ¨ ìºì‹œ ë¬´íš¨í™”"""
        patterns = [
            f'get_category_list:*',
            f'get_cat:*{category_id}*',
        ]

        total_deleted = 0
        for pattern in patterns:
            count = await self.invalidate_pattern(pattern)
            total_deleted += count

        return total_deleted


# ì‚¬ìš© ì˜ˆì‹œ
cache_manager = CacheManager(redis_url='redis://localhost:6379')

@cache_manager.cache(ttl=3600)
async def get_item_view(item_id: str):
    # ì‹¤ì œ API í˜¸ì¶œ
    ...

# ëª…ì‹œì  ë¬´íš¨í™”
await cache_manager.invalidate_product('12345')
```

---

## P2: ê°€ê²© ë™ê¸°í™” Delta ê°ì§€

### ë¬¸ì œì 

ì „ì²´ ìƒí’ˆì„ ì£¼ê¸°ì ìœ¼ë¡œ ë™ê¸°í™” â†’ ë¹„íš¨ìœ¨ì 

### í•´ê²°ì±…: ë³€ê²½ ê°ì§€ + ì´ë²¤íŠ¸ ê¸°ë°˜

```python
# app/workers/sync.py

from datetime import datetime, timedelta
from sqlalchemy import select
from app.models.product import Product, ProductRegistration

async def detect_price_changes():
    """ê°€ê²© ë³€ê²½ ê°ì§€ (Delta)"""

    # 1ì‹œê°„ ì´ë‚´ ì—…ë°ì´íŠ¸ëœ ìƒí’ˆë§Œ
    cutoff_time = datetime.now() - timedelta(hours=1)

    stmt = select(Product).where(
        Product.updated_at > cutoff_time
    )

    products = await db.execute(stmt)

    changed_products = []

    for product in products.scalars():
        # ìµœì‹  ë°ì´í„° ì¡°íšŒ
        fresh_data = await domeggook_client.get_item_view(
            product.domeggook_item_id
        )

        # ë³€ê²½ ê°ì§€
        if fresh_data['price'] != product.price:
            changed_products.append({
                'product_id': product.id,
                'old_price': product.price,
                'new_price': fresh_data['price']
            })

            # DB ì—…ë°ì´íŠ¸
            product.price = fresh_data['price']
            await db.commit()

    # ë³€ê²½ëœ ìƒí’ˆë§Œ ë™ê¸°í™” íì— ì¶”ê°€
    for change in changed_products:
        await sync_queue.enqueue(change)

    return len(changed_products)
```

---

## P3: Prometheus ë©”íŠ¸ë¦­ ìƒì„¸í™”

```python
# app/utils/metrics.py

from prometheus_client import Counter, Histogram, Gauge, Info

# 1. ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­
products_registered_total = Counter(
    'storebridge_products_registered_total',
    'Total products registered',
    ['status', 'source']  # success/failed, batch/manual
)

registration_success_rate = Gauge(
    'storebridge_registration_success_rate',
    'Registration success rate (last 1 hour)'
)

rejection_reason_count = Counter(
    'storebridge_rejection_reason_total',
    'Total rejections by reason',
    ['reason']  # CATEGORY_MISMATCH, FORBIDDEN_WORD, etc.
)

# 2. ì„±ëŠ¥ ë©”íŠ¸ë¦­
api_call_duration_seconds = Histogram(
    'storebridge_api_call_duration_seconds',
    'API call duration',
    ['api_name', 'method'],  # domeggook/naver, GET/POST
    buckets=[0.1, 0.5, 1, 2, 5, 10, 30]
)

image_processing_duration_seconds = Histogram(
    'storebridge_image_processing_duration_seconds',
    'Image processing duration',
    buckets=[0.5, 1, 2, 5, 10]
)

# 3. ì¸í”„ë¼ ë©”íŠ¸ë¦­
queue_depth = Gauge(
    'storebridge_queue_depth',
    'Current queue depth',
    ['queue_name']  # normal/batch/sync/review
)

rate_limit_remaining = Gauge(
    'storebridge_rate_limit_remaining',
    'Remaining rate limit',
    ['api_name']  # domeggook/naver
)

cache_hit_rate = Gauge(
    'storebridge_cache_hit_rate',
    'Cache hit rate (last 5 minutes)'
)

# 4. ì• í”Œë¦¬ì¼€ì´ì…˜ ì •ë³´
app_info = Info(
    'storebridge_app',
    'Application information'
)
app_info.info({
    'version': '1.0.0',
    'python_version': '3.11',
    'environment': 'production'
})
```

---

ì´ìƒìœ¼ë¡œ P0~P3 ê°œì„ ì‚¬í•­ ìƒì„¸ ì„¤ê³„ ì™„ë£Œ! ë‹¤ìŒì€ DB ìŠ¤í‚¤ë§ˆ ìƒì„¸ ì„¤ê³„ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤.
