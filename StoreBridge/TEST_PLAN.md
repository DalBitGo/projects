# StoreBridge Test Plan
**í…ŒìŠ¤íŠ¸ ê³„íšì„œ**

---

## ğŸ“‹ Table of Contents

1. [Test Strategy Overview](#test-strategy-overview)
2. [Test Pyramid & Coverage Targets](#test-pyramid--coverage-targets)
3. [Test Environments](#test-environments)
4. [Unit Testing Plan](#unit-testing-plan)
5. [Integration Testing Plan](#integration-testing-plan)
6. [End-to-End Testing Plan](#end-to-end-testing-plan)
7. [Performance & Load Testing Plan](#performance--load-testing-plan)
8. [Test Data Management](#test-data-management)
9. [Mocking Strategy](#mocking-strategy)
10. [CI/CD Integration](#cicd-integration)
11. [Test Execution Guide](#test-execution-guide)
12. [Appendix: Sample Test Cases](#appendix-sample-test-cases)

---

## 1. Test Strategy Overview

### 1.1 Testing Philosophy

StoreBridgeì˜ í…ŒìŠ¤íŠ¸ ì „ëµì€ ë‹¤ìŒ ì›ì¹™ì„ ë”°ë¦…ë‹ˆë‹¤:

| ì›ì¹™ | ì„¤ëª… |
|------|------|
| **Fast Feedback** | ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ëŠ” 1ì´ˆ ì´ë‚´, í†µí•© í…ŒìŠ¤íŠ¸ëŠ” 10ì´ˆ ì´ë‚´ ì‹¤í–‰ |
| **Deterministic** | ë™ì¼í•œ ì…ë ¥ì€ í•­ìƒ ë™ì¼í•œ ê²°ê³¼ (ì‹œê°„/ë„¤íŠ¸ì›Œí¬ ë…ë¦½ì ) |
| **Isolated** | ê° í…ŒìŠ¤íŠ¸ëŠ” ë…ë¦½ì ì´ë©° ìˆœì„œì— ë¬´ê´€ |
| **Maintainable** | í…ŒìŠ¤íŠ¸ ì½”ë“œë„ í”„ë¡œë•ì…˜ ì½”ë“œì™€ ë™ì¼í•œ í’ˆì§ˆ ê¸°ì¤€ ì ìš© |

### 1.2 Test Scope

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Test Scope                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ âœ… Included:                                                â”‚
â”‚   - Rate Limiter atomic operations                          â”‚
â”‚   - Option Mapper parsing logic (1D/2D/3D)                  â”‚
â”‚   - Validators (category, image, forbidden words)           â”‚
â”‚   - State machine transitions                               â”‚
â”‚   - API integration (Domeggook, Naver)                      â”‚
â”‚   - Image processing pipeline                               â”‚
â”‚   - Database queries & triggers                             â”‚
â”‚   - Error handling & retry logic                            â”‚
â”‚                                                              â”‚
â”‚ âŒ Excluded:                                                â”‚
â”‚   - Third-party library internals (httpx, SQLAlchemy)       â”‚
â”‚   - Infrastructure (Kubernetes, Redis, PostgreSQL)          â”‚
â”‚   - UI/Frontend (out of scope - backend only)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. Test Pyramid & Coverage Targets

### 2.1 Test Pyramid

```
              /\
             /  \        E2E Tests (10%)
            /    \       - ì „ì²´ ë“±ë¡ í”Œë¡œìš°
           /______\      - ì‹¤íŒ¨ ì‹œë‚˜ë¦¬ì˜¤
          /        \
         /          \    Integration Tests (30%)
        /            \   - API í†µí•©
       /______________\  - DB í†µí•©
      /                \
     /                  \ Unit Tests (60%)
    /                    \ - ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§
   /______________________\ - Validators, Transformers
```

### 2.2 Coverage Targets

| Test Type | Target Coverage | Execution Time | Frequency |
|-----------|----------------|----------------|-----------|
| **Unit Tests** | 85% (line), 80% (branch) | < 1 min | Every commit |
| **Integration Tests** | 70% (critical paths) | < 5 min | Every PR |
| **E2E Tests** | 100% (happy paths) | < 15 min | Before merge |
| **Load Tests** | N/A (performance metrics) | 30 min | Weekly + before release |

### 2.3 Critical Paths (100% Coverage Required)

1. **Rate Limiter** - ë„¤ì´ë²„ API 2 TPS ì œì•½ ìœ„ë°˜ ë°©ì§€ (P0)
2. **State Machine** - ì˜ëª»ëœ ìƒíƒœ ì „í™˜ ë°©ì§€ (P0)
3. **Image Upload** - ë„¤ì´ë²„ ì—…ë¡œë“œ ì‹¤íŒ¨ ì²˜ë¦¬ (P0)
4. **Option Mapper** - ì˜ëª»ëœ ì˜µì…˜ êµ¬ì¡°ë¡œ ì¸í•œ ë“±ë¡ ì‹¤íŒ¨ ë°©ì§€ (P1)

---

## 3. Test Environments

### 3.1 Environment Matrix

| Environment | Purpose | Database | Redis | External APIs |
|-------------|---------|----------|-------|---------------|
| **local** | ê°œë°œì ë¡œì»¬ ê°œë°œ | PostgreSQL (Docker) | Redis (Docker) | VCR.py (mocked) |
| **ci** | GitHub Actions | PostgreSQL (service) | Redis (service) | VCR.py (mocked) |
| **staging** | í†µí•© í…ŒìŠ¤íŠ¸ | RDS (isolated) | ElastiCache | **Real APIs** (sandbox) |
| **production** | N/A (no tests) | - | - | - |

### 3.2 Environment Configuration

**local / ci** (.env.test):
```bash
DATABASE_URL=postgresql://test:test@localhost:5432/storebridge_test
REDIS_URL=redis://localhost:6379/1
DOMEGGOOK_API_KEY=test_key_12345
NAVER_CLIENT_ID=test_client
NAVER_CLIENT_SECRET=test_secret
ENVIRONMENT=test
VCR_MODE=once  # once: ì²« ì‹¤í–‰ ì‹œ ë…¹í™”, ì´í›„ ì¬ìƒ
```

**staging** (.env.staging):
```bash
DATABASE_URL=postgresql://user:pass@staging-db.rds.amazonaws.com/storebridge_staging
REDIS_URL=redis://staging-redis.elasticache.amazonaws.com:6379
DOMEGGOOK_API_KEY=${DOMEGGOOK_SANDBOX_KEY}  # Sandbox API key
NAVER_CLIENT_ID=${NAVER_STAGING_CLIENT_ID}
NAVER_CLIENT_SECRET=${NAVER_STAGING_SECRET}
ENVIRONMENT=staging
VCR_MODE=none  # ì‹¤ì œ API í˜¸ì¶œ
```

---

## 4. Unit Testing Plan

### 4.1 Test Structure

```
tests/
â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ test_rate_limiter.py          # Rate Limiter (P0)
â”‚   â”‚   â”œâ”€â”€ test_option_mapper.py         # Option parsing (P1)
â”‚   â”‚   â””â”€â”€ test_image_processor.py       # Image pipeline (P1)
â”‚   â”œâ”€â”€ validators/
â”‚   â”‚   â”œâ”€â”€ test_product_validator.py     # ìƒí’ˆ ê²€ì¦
â”‚   â”‚   â”œâ”€â”€ test_category_validator.py    # ì¹´í…Œê³ ë¦¬ ë§¤í•‘
â”‚   â”‚   â””â”€â”€ test_forbidden_word_validator.py  # ê¸ˆì¹™ì–´
â”‚   â”œâ”€â”€ transformers/
â”‚   â”‚   â”œâ”€â”€ test_product_transformer.py   # ë°ì´í„° ë³€í™˜
â”‚   â”‚   â””â”€â”€ test_image_transformer.py     # ì´ë¯¸ì§€ ë³€í™˜
â”‚   â”œâ”€â”€ workflows/
â”‚   â”‚   â””â”€â”€ test_registration_workflow.py # State machine
â”‚   â””â”€â”€ connectors/
â”‚       â”œâ”€â”€ test_domeggook_client.py      # ë„ë§¤ê¾¹ í´ë¼ì´ì–¸íŠ¸
â”‚       â””â”€â”€ test_naver_client.py          # ë„¤ì´ë²„ í´ë¼ì´ì–¸íŠ¸
```

### 4.2 Rate Limiter Tests (P0 - Critical)

**í…ŒìŠ¤íŠ¸ ëª©í‘œ:**
- Lua ìŠ¤í¬ë¦½íŠ¸ì˜ atomic operation ê²€ì¦
- Race condition ë°©ì§€ í™•ì¸
- Burst Max ê¸°ëŠ¥ ë™ì‘ í™•ì¸

**tests/unit/services/test_rate_limiter.py:**

```python
import pytest
import asyncio
from unittest.mock import AsyncMock, MagicMock
from app.services.rate_limiter import NaverRateLimiter

class TestNaverRateLimiter:
    """ë„¤ì´ë²„ API Rate Limiter í…ŒìŠ¤íŠ¸ (2 TPS ì œì•½)"""

    @pytest.fixture
    def redis_mock(self):
        """Redis mock with eval support"""
        redis = AsyncMock()
        redis.eval = AsyncMock()
        return redis

    @pytest.fixture
    def limiter(self, redis_mock):
        return NaverRateLimiter(redis=redis_mock, max_tps=2)

    # ===== Happy Path =====

    @pytest.mark.asyncio
    async def test_acquire_success_within_limit(self, limiter, redis_mock):
        """2 TPS ì´ë‚´ ìš”ì²­ì€ ì„±ê³µ"""
        redis_mock.eval.return_value = 1  # Lua script returns 1 (success)

        result = await limiter.acquire()

        assert result is True
        redis_mock.eval.assert_called_once()

    @pytest.mark.asyncio
    async def test_acquire_blocked_over_limit(self, limiter, redis_mock):
        """2 TPS ì´ˆê³¼ ìš”ì²­ì€ ì°¨ë‹¨"""
        redis_mock.eval.return_value = 0  # Lua script returns 0 (blocked)

        result = await limiter.acquire()

        assert result is False

    # ===== Race Condition Test =====

    @pytest.mark.asyncio
    async def test_concurrent_acquire_no_race_condition(self, limiter, redis_mock):
        """ë™ì‹œ ìš”ì²­ ì‹œ Race Condition ì—†ìŒ (Lua atomic)"""
        call_count = 0

        async def mock_eval(*args, **kwargs):
            nonlocal call_count
            call_count += 1
            if call_count <= 2:
                return 1  # First 2 succeed
            else:
                return 0  # Rest blocked

        redis_mock.eval.side_effect = mock_eval

        # 10ê°œ ë™ì‹œ ìš”ì²­ (max_tps=2ì´ë¯€ë¡œ 2ê°œë§Œ ì„±ê³µí•´ì•¼ í•¨)
        tasks = [limiter.acquire() for _ in range(10)]
        results = await asyncio.gather(*tasks)

        success_count = sum(results)
        assert success_count == 2, "Only 2 requests should succeed"
        assert redis_mock.eval.call_count == 10

    # ===== Burst Max Test =====

    @pytest.mark.asyncio
    async def test_burst_max_allows_temporary_spike(self, redis_mock):
        """Burst MaxëŠ” ì¼ì‹œì  ìŠ¤íŒŒì´í¬ í—ˆìš© (3 TPS)"""
        limiter = NaverRateLimiter(
            redis=redis_mock,
            max_tps=2,
            burst_max=3  # ì¼ì‹œì ìœ¼ë¡œ 3 TPS í—ˆìš©
        )

        call_count = 0

        async def mock_eval(*args, **kwargs):
            nonlocal call_count
            call_count += 1
            if call_count <= 3:  # Burst allows 3
                return 1
            else:
                return 0

        redis_mock.eval.side_effect = mock_eval

        tasks = [limiter.acquire() for _ in range(5)]
        results = await asyncio.gather(*tasks)

        assert sum(results) == 3, "Burst allows 3 requests temporarily"

    # ===== TTL Test =====

    @pytest.mark.asyncio
    async def test_redis_key_expires_after_ttl(self, limiter, redis_mock):
        """Redis keyëŠ” TTL í›„ ìë™ ë§Œë£Œ"""
        redis_mock.eval.return_value = 1

        await limiter.acquire()

        # Lua ìŠ¤í¬ë¦½íŠ¸ì— TTLì´ ì „ë‹¬ë˜ëŠ”ì§€ í™•ì¸
        call_args = redis_mock.eval.call_args
        assert call_args[0][2] == limiter.ttl  # ARGV[2] = ttl

    # ===== Error Handling =====

    @pytest.mark.asyncio
    async def test_redis_connection_error_raises_exception(self, limiter, redis_mock):
        """Redis ì—°ê²° ì˜¤ë¥˜ ì‹œ ì˜ˆì™¸ ë°œìƒ"""
        redis_mock.eval.side_effect = ConnectionError("Redis unavailable")

        with pytest.raises(ConnectionError):
            await limiter.acquire()

    @pytest.mark.asyncio
    async def test_lua_script_error_raises_exception(self, limiter, redis_mock):
        """Lua ìŠ¤í¬ë¦½íŠ¸ ì˜¤ë¥˜ ì‹œ ì˜ˆì™¸ ë°œìƒ"""
        redis_mock.eval.side_effect = Exception("Lua script error")

        with pytest.raises(Exception):
            await limiter.acquire()
```

**ì‹¤í–‰ ì‹œê°„ ëª©í‘œ:** < 1ì´ˆ (ëª¨ë“  Rate Limiter í…ŒìŠ¤íŠ¸)

---

### 4.3 Option Mapper Tests (P1 - High)

**í…ŒìŠ¤íŠ¸ ëª©í‘œ:**
- 1D/2D/3D ì˜µì…˜ íŒŒì‹± ì •í™•ë„
- Separator ìë™ ê°ì§€
- Edge cases (ê³µë°±, íŠ¹ìˆ˜ë¬¸ì, ë¹ˆ ê°’)

**tests/unit/services/test_option_mapper.py:**

```python
import pytest
from app.services.option_mapper import OptionMapper

class TestOptionMapper:
    """ì˜µì…˜ ë§¤í•‘ í…ŒìŠ¤íŠ¸ (ë„ë§¤ê¾¹ â†’ ë„¤ì´ë²„)"""

    @pytest.fixture
    def mapper(self):
        return OptionMapper()

    # ===== 1D Options (Simple) =====

    def test_parse_1d_simple_options(self, mapper):
        """ë‹¨ì¼ ì°¨ì› ì˜µì…˜ (ìƒ‰ìƒë§Œ)"""
        raw_options = ["ë¸”ë™", "í™”ì´íŠ¸", "ë„¤ì´ë¹„"]

        result = mapper.parse(raw_options)

        assert result["type"] == "SIMPLE"
        assert result["dimension_name"] == "ìƒ‰ìƒ"
        assert result["values"] == ["ë¸”ë™", "í™”ì´íŠ¸", "ë„¤ì´ë¹„"]

    # ===== 2D Options (Combination) =====

    def test_parse_2d_combination_with_dash(self, mapper):
        """2ì°¨ì› ì¡°í•© ì˜µì…˜ (ìƒ‰ìƒ-ì‚¬ì´ì¦ˆ, separator='-')"""
        raw_options = ["ë¸”ë™-S", "ë¸”ë™-M", "í™”ì´íŠ¸-S", "í™”ì´íŠ¸-M"]

        result = mapper.parse(raw_options)

        assert result["type"] == "COMBINATION"
        assert result["separator"] == "-"
        assert len(result["dimensions"]) == 2

        # Dimension 1: ìƒ‰ìƒ
        assert result["dimensions"][0]["name"] == "ìƒ‰ìƒ"
        assert set(result["dimensions"][0]["values"]) == {"ë¸”ë™", "í™”ì´íŠ¸"}

        # Dimension 2: ì‚¬ì´ì¦ˆ
        assert result["dimensions"][1]["name"] == "ì‚¬ì´ì¦ˆ"
        assert set(result["dimensions"][1]["values"]) == {"S", "M"}

        # Combinations
        assert len(result["combinations"]) == 4
        assert {"ìƒ‰ìƒ": "ë¸”ë™", "ì‚¬ì´ì¦ˆ": "S"} in result["combinations"]

    def test_parse_2d_combination_with_slash(self, mapper):
        """2ì°¨ì› ì¡°í•© ì˜µì…˜ (separator='/')"""
        raw_options = ["ë ˆë“œ/L", "ë¸”ë£¨/XL"]

        result = mapper.parse(raw_options)

        assert result["type"] == "COMBINATION"
        assert result["separator"] == "/"
        assert len(result["dimensions"]) == 2

    # ===== 3D Options (Combination) =====

    def test_parse_3d_combination(self, mapper):
        """3ì°¨ì› ì¡°í•© ì˜µì…˜ (ìƒ‰ìƒ-ì‚¬ì´ì¦ˆ-ì¬ì§ˆ)"""
        raw_options = [
            "ë¸”ë™-S-ë©´",
            "ë¸”ë™-M-ë©´",
            "ë¸”ë™-M-í´ë¦¬",
            "í™”ì´íŠ¸-S-ë©´"
        ]

        result = mapper.parse(raw_options)

        assert result["type"] == "COMBINATION"
        assert len(result["dimensions"]) == 3
        assert result["dimensions"][0]["name"] == "ìƒ‰ìƒ"
        assert result["dimensions"][1]["name"] == "ì‚¬ì´ì¦ˆ"
        assert result["dimensions"][2]["name"] == "ì¬ì§ˆ"

    # ===== Edge Cases =====

    def test_empty_options_returns_empty_result(self, mapper):
        """ë¹ˆ ì˜µì…˜ ë¦¬ìŠ¤íŠ¸"""
        result = mapper.parse([])

        assert result["type"] == "EMPTY"
        assert result["dimensions"] == []

    def test_options_with_whitespace(self, mapper):
        """ê³µë°± í¬í•¨ ì˜µì…˜"""
        raw_options = [" ë¸”ë™ - S ", " í™”ì´íŠ¸ - M "]

        result = mapper.parse(raw_options)

        # ê³µë°± ì œê±° í›„ íŒŒì‹±
        assert result["type"] == "COMBINATION"
        assert result["dimensions"][0]["values"] == ["ë¸”ë™", "í™”ì´íŠ¸"]
        assert result["dimensions"][1]["values"] == ["S", "M"]

    def test_options_with_special_characters(self, mapper):
        """íŠ¹ìˆ˜ë¬¸ì í¬í•¨ ì˜µì…˜"""
        raw_options = ["ë¸”ë™(ë¬´ê´‘)-S", "í™”ì´íŠ¸(ê´‘íƒ)-M"]

        result = mapper.parse(raw_options)

        assert result["type"] == "COMBINATION"
        assert "ë¸”ë™(ë¬´ê´‘)" in result["dimensions"][0]["values"]

    def test_inconsistent_separator_raises_error(self, mapper):
        """ì¼ê´€ì„± ì—†ëŠ” separator"""
        raw_options = ["ë¸”ë™-S", "í™”ì´íŠ¸/M"]  # Mixed separators

        with pytest.raises(ValueError, match="Inconsistent separator"):
            mapper.parse(raw_options)

    # ===== Naver Format Conversion =====

    def test_to_naver_format_2d(self, mapper):
        """ë„¤ì´ë²„ API í˜•ì‹ ë³€í™˜"""
        raw_options = ["ë¸”ë™-S", "í™”ì´íŠ¸-M"]
        parsed = mapper.parse(raw_options)

        naver_format = mapper.to_naver_format(parsed)

        assert naver_format["optionType"] == "COMBINATION"
        assert len(naver_format["optionCombinations"]) == 2
        assert naver_format["optionCombinations"][0] == {
            "optionName1": "ìƒ‰ìƒ",
            "optionValue1": "ë¸”ë™",
            "optionName2": "ì‚¬ì´ì¦ˆ",
            "optionValue2": "S",
            "stockQuantity": 0,  # Default
            "price": 0  # To be filled by transformer
        }
```

**ì‹¤í–‰ ì‹œê°„ ëª©í‘œ:** < 500ms

---

### 4.4 Validator Tests

**tests/unit/validators/test_forbidden_word_validator.py:**

```python
import pytest
from app.validators.forbidden_word_validator import ForbiddenWordValidator

class TestForbiddenWordValidator:
    """ê¸ˆì¹™ì–´ ê²€ì¦ í…ŒìŠ¤íŠ¸"""

    @pytest.fixture
    def validator(self):
        return ForbiddenWordValidator(
            forbidden_words=["ë³‘ ì¹˜ë£Œ", "ì˜ì•½í’ˆ", "100% íš¨ê³¼", "ë¬´ì¡°ê±´"]
        )

    def test_clean_text_passes(self, validator):
        """ê¸ˆì¹™ì–´ ì—†ëŠ” í…ìŠ¤íŠ¸ í†µê³¼"""
        result = validator.validate("ê³ í’ˆì§ˆ ë©´ í‹°ì…”ì¸ ì…ë‹ˆë‹¤")

        assert result.is_valid is True
        assert result.errors == []

    def test_forbidden_word_detected(self, validator):
        """ê¸ˆì¹™ì–´ ê°ì§€"""
        result = validator.validate("ì´ ì œí’ˆì€ ë³‘ ì¹˜ë£Œì— íš¨ê³¼ì ì…ë‹ˆë‹¤")

        assert result.is_valid is False
        assert "ë³‘ ì¹˜ë£Œ" in result.errors[0]

    def test_multiple_forbidden_words(self, validator):
        """ì—¬ëŸ¬ ê¸ˆì¹™ì–´ ê°ì§€"""
        text = "100% íš¨ê³¼ë¥¼ ë¬´ì¡°ê±´ ë³´ì¥í•©ë‹ˆë‹¤"
        result = validator.validate(text)

        assert result.is_valid is False
        assert len(result.errors) == 2

    def test_case_insensitive_matching(self, validator):
        """ëŒ€ì†Œë¬¸ì ë¬´ê´€ ë§¤ì¹­"""
        result = validator.validate("ì´ ì œí’ˆì€ ì˜ì•½í’ˆì´ ì•„ë‹™ë‹ˆë‹¤")

        assert result.is_valid is False  # "ì˜ì•½í’ˆ" detected
```

---

### 4.5 State Machine Tests

**tests/unit/workflows/test_registration_workflow.py:**

```python
import pytest
from app.workflows.registration_workflow import RegistrationStateMachine, State

class TestRegistrationStateMachine:
    """ë“±ë¡ í”Œë¡œìš° State Machine í…ŒìŠ¤íŠ¸"""

    @pytest.fixture
    def state_machine(self):
        return RegistrationStateMachine()

    # ===== Valid Transitions =====

    def test_pending_to_validated(self, state_machine):
        """PENDING â†’ VALIDATED (valid)"""
        state_machine.current_state = State.PENDING

        state_machine.transition_to(State.VALIDATED)

        assert state_machine.current_state == State.VALIDATED

    def test_validated_to_uploading(self, state_machine):
        """VALIDATED â†’ UPLOADING (valid)"""
        state_machine.current_state = State.VALIDATED

        state_machine.transition_to(State.UPLOADING)

        assert state_machine.current_state == State.UPLOADING

    def test_uploading_to_registering(self, state_machine):
        """UPLOADING â†’ REGISTERING (valid)"""
        state_machine.current_state = State.UPLOADING

        state_machine.transition_to(State.REGISTERING)

        assert state_machine.current_state == State.REGISTERING

    def test_registering_to_completed(self, state_machine):
        """REGISTERING â†’ COMPLETED (valid)"""
        state_machine.current_state = State.REGISTERING

        state_machine.transition_to(State.COMPLETED)

        assert state_machine.current_state == State.COMPLETED

    # ===== Invalid Transitions =====

    def test_pending_to_uploading_invalid(self, state_machine):
        """PENDING â†’ UPLOADING (invalid - must validate first)"""
        state_machine.current_state = State.PENDING

        with pytest.raises(ValueError, match="Invalid state transition"):
            state_machine.transition_to(State.UPLOADING)

    def test_completed_to_any_invalid(self, state_machine):
        """COMPLETED â†’ * (invalid - terminal state)"""
        state_machine.current_state = State.COMPLETED

        with pytest.raises(ValueError, match="Cannot transition from terminal state"):
            state_machine.transition_to(State.PENDING)

    # ===== Retry Logic =====

    def test_retrying_to_validated_on_retry(self, state_machine):
        """RETRYING â†’ VALIDATED (ì¬ì‹œë„ ì‹œ ê²€ì¦ ë‹¨ê³„ë¡œ)"""
        state_machine.current_state = State.RETRYING

        state_machine.transition_to(State.VALIDATED)

        assert state_machine.current_state == State.VALIDATED

    # ===== Manual Review =====

    def test_any_state_to_manual_review_allowed(self, state_machine):
        """ëª¨ë“  ìƒíƒœ â†’ MANUAL_REVIEW (allowed)"""
        for state in [State.PENDING, State.VALIDATED, State.UPLOADING]:
            state_machine.current_state = state
            state_machine.transition_to(State.MANUAL_REVIEW)
            assert state_machine.current_state == State.MANUAL_REVIEW
            state_machine.reset()  # Reset for next iteration
```

---

## 5. Integration Testing Plan

### 5.1 Test Structure

```
tests/
â”œâ”€â”€ integration/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ test_domeggook_integration.py    # ë„ë§¤ê¾¹ API ì‹¤ì œ í˜¸ì¶œ
â”‚   â”‚   â””â”€â”€ test_naver_integration.py        # ë„¤ì´ë²„ API ì‹¤ì œ í˜¸ì¶œ
â”‚   â”œâ”€â”€ database/
â”‚   â”‚   â”œâ”€â”€ test_product_repository.py       # DB CRUD
â”‚   â”‚   â”œâ”€â”€ test_state_machine_triggers.py   # DB triggers
â”‚   â”‚   â””â”€â”€ test_query_performance.py        # Query optimization
â”‚   â”œâ”€â”€ cache/
â”‚   â”‚   â””â”€â”€ test_redis_integration.py        # Redis cache
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ test_full_registration_flow.py   # ì „ì²´ í”Œë¡œìš°
```

### 5.2 Domeggook API Integration Tests

**tests/integration/api/test_domeggook_integration.py:**

```python
import pytest
import vcr
from app.connectors.domeggook_client import DomeggookClient

# VCR.pyë¡œ HTTP ìš”ì²­/ì‘ë‹µ ë…¹í™”/ì¬ìƒ
my_vcr = vcr.VCR(
    cassette_library_dir='tests/fixtures/vcr_cassettes',
    record_mode='once',  # ì²« ì‹¤í–‰ ì‹œ ë…¹í™”, ì´í›„ ì¬ìƒ
    match_on=['uri', 'method'],
    filter_headers=['Authorization']  # API key ì œê±°
)

class TestDomeggookIntegration:
    """ë„ë§¤ê¾¹ API í†µí•© í…ŒìŠ¤íŠ¸"""

    @pytest.fixture
    def client(self):
        return DomeggookClient(api_key="test_key")

    @my_vcr.use_cassette('domeggook_get_item_list.yaml')
    @pytest.mark.asyncio
    async def test_get_item_list_returns_products(self, client):
        """ìƒí’ˆ ëª©ë¡ ì¡°íšŒ (ì‹¤ì œ API í˜¸ì¶œ)"""
        result = await client.get_item_list(
            page=1,
            page_size=10,
            category="íŒ¨ì…˜ì˜ë¥˜"
        )

        assert result["success"] is True
        assert len(result["items"]) > 0
        assert "item_id" in result["items"][0]
        assert "item_name" in result["items"][0]

    @my_vcr.use_cassette('domeggook_get_item_view.yaml')
    @pytest.mark.asyncio
    async def test_get_item_view_returns_detail(self, client):
        """ìƒí’ˆ ìƒì„¸ ì¡°íšŒ"""
        result = await client.get_item_view(item_id="12345")

        assert result["success"] is True
        assert result["item"]["item_id"] == "12345"
        assert "images" in result["item"]
        assert "options" in result["item"]

    @my_vcr.use_cassette('domeggook_rate_limit_429.yaml')
    @pytest.mark.asyncio
    async def test_rate_limit_error_raises_exception(self, client):
        """Rate limit ì´ˆê³¼ ì‹œ ì˜ˆì™¸ ë°œìƒ (429)"""
        with pytest.raises(Exception, match="Rate limit exceeded"):
            # 180íšŒ ì—°ì† í˜¸ì¶œ (rate limit ë„ë‹¬)
            for _ in range(181):
                await client.get_item_list(page=1, page_size=1)

    @my_vcr.use_cassette('domeggook_encoding_euc_kr.yaml')
    @pytest.mark.asyncio
    async def test_euc_kr_encoding_handled_correctly(self, client):
        """EUC-KR ì¸ì½”ë”© ì²˜ë¦¬"""
        result = await client.get_item_list(keyword="í•œê¸€ìƒí’ˆëª…")

        # í•œê¸€ì´ ê¹¨ì§€ì§€ ì•Šê³  ì •ìƒ íŒŒì‹±ë˜ì—ˆëŠ”ì§€ í™•ì¸
        assert "í•œê¸€" in str(result["items"])
```

**VCR Cassette ì˜ˆì‹œ** (tests/fixtures/vcr_cassettes/domeggook_get_item_list.yaml):

```yaml
version: 1
interactions:
- request:
    uri: https://openapi.domeggook.com/getItemList
    method: GET
    body: null
    headers:
      User-Agent: [StoreBridge/1.0]
  response:
    status: {code: 200, message: OK}
    body:
      string: '{"success":true,"items":[{"item_id":"12345","item_name":"ë©´ í‹°ì…”ì¸ "}]}'
    headers:
      Content-Type: [application/json; charset=utf-8]
```

---

### 5.3 Naver API Integration Tests

**tests/integration/api/test_naver_integration.py:**

```python
import pytest
import vcr
from app.connectors.naver_client import NaverClient

my_vcr = vcr.VCR(
    cassette_library_dir='tests/fixtures/vcr_cassettes',
    record_mode='once',
    filter_headers=['Authorization']
)

class TestNaverIntegration:
    """ë„¤ì´ë²„ Commerce API í†µí•© í…ŒìŠ¤íŠ¸"""

    @pytest.fixture
    def client(self):
        return NaverClient(
            client_id="test_client",
            client_secret="test_secret"
        )

    @my_vcr.use_cassette('naver_upload_image.yaml')
    @pytest.mark.asyncio
    async def test_upload_image_returns_url(self, client):
        """ì´ë¯¸ì§€ ì—…ë¡œë“œ (ë„¤ì´ë²„ CDN)"""
        image_data = b"fake_image_data"

        result = await client.upload_image(image_data)

        assert result["success"] is True
        assert result["image_url"].startswith("https://shopping-phinf.pstatic.net")

    @my_vcr.use_cassette('naver_register_product.yaml')
    @pytest.mark.asyncio
    async def test_register_product_returns_product_id(self, client):
        """ìƒí’ˆ ë“±ë¡"""
        product_data = {
            "originProduct": {
                "name": "í…ŒìŠ¤íŠ¸ ìƒí’ˆ",
                "salePrice": 10000,
                "categoryId": "50000000",
                "images": [{"url": "https://example.com/image.jpg"}],
                "detailContent": "ìƒí’ˆ ìƒì„¸ ì„¤ëª…"
            }
        }

        result = await client.register_product(product_data)

        assert result["success"] is True
        assert "originProductNo" in result

    @my_vcr.use_cassette('naver_rate_limit_429.yaml')
    @pytest.mark.asyncio
    async def test_rate_limit_2_tps_enforced(self, client):
        """2 TPS Rate limit í™•ì¸"""
        import time

        # 1ì´ˆì— 3ë²ˆ í˜¸ì¶œ ì‹œë„ (2 TPS ì´ˆê³¼)
        start = time.time()
        results = []
        for i in range(3):
            try:
                result = await client.get_product("test_id")
                results.append("success")
            except Exception as e:
                if "429" in str(e):
                    results.append("rate_limited")

        elapsed = time.time() - start
        assert elapsed < 1.5  # 1ì´ˆ ë‚´ ì‹¤í–‰
        assert results.count("rate_limited") > 0  # ì ì–´ë„ 1ê°œëŠ” ì°¨ë‹¨ë¨
```

---

### 5.4 Database Integration Tests

**tests/integration/database/test_state_machine_triggers.py:**

```python
import pytest
from sqlalchemy.ext.asyncio import AsyncSession
from app.models import ProductRegistration, State

class TestStateMachineTriggers:
    """DB íŠ¸ë¦¬ê±°ë¥¼ í†µí•œ State Machine ê²€ì¦"""

    @pytest.mark.asyncio
    async def test_valid_transition_updates_state(self, db: AsyncSession):
        """Valid transition: DB update ì„±ê³µ"""
        registration = ProductRegistration(
            product_id="uuid-123",
            state=State.PENDING
        )
        db.add(registration)
        await db.commit()

        # PENDING â†’ VALIDATED (valid)
        registration.state = State.VALIDATED
        await db.commit()  # Should succeed

        await db.refresh(registration)
        assert registration.state == State.VALIDATED

    @pytest.mark.asyncio
    async def test_invalid_transition_raises_db_error(self, db: AsyncSession):
        """Invalid transition: DB íŠ¸ë¦¬ê±°ê°€ ì˜ˆì™¸ ë°œìƒ"""
        registration = ProductRegistration(
            product_id="uuid-123",
            state=State.PENDING
        )
        db.add(registration)
        await db.commit()

        # PENDING â†’ UPLOADING (invalid - must validate first)
        registration.state = State.UPLOADING

        with pytest.raises(Exception, match="Invalid state transition"):
            await db.commit()

    @pytest.mark.asyncio
    async def test_updated_at_auto_updated_on_change(self, db: AsyncSession):
        """updated_at ìë™ ê°±ì‹ """
        import asyncio
        from datetime import datetime

        registration = ProductRegistration(
            product_id="uuid-123",
            state=State.PENDING
        )
        db.add(registration)
        await db.commit()

        original_updated_at = registration.updated_at
        await asyncio.sleep(0.1)  # 0.1ì´ˆ ëŒ€ê¸°

        registration.state = State.VALIDATED
        await db.commit()
        await db.refresh(registration)

        assert registration.updated_at > original_updated_at
```

**tests/integration/database/test_query_performance.py:**

```python
import pytest
from sqlalchemy import select
from app.models import ProductRegistration, State

class TestQueryPerformance:
    """ì¿¼ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ (ì¸ë±ìŠ¤ íš¨ìœ¨ì„±)"""

    @pytest.mark.asyncio
    async def test_pending_state_query_uses_partial_index(self, db):
        """PENDING ìƒíƒœ ì¡°íšŒëŠ” Partial Index ì‚¬ìš©"""
        # 1000ê°œ ë ˆì½”ë“œ ìƒì„± (500 PENDING, 500 COMPLETED)
        registrations = []
        for i in range(500):
            registrations.append(ProductRegistration(
                product_id=f"uuid-{i}",
                state=State.PENDING
            ))
            registrations.append(ProductRegistration(
                product_id=f"uuid-{i+500}",
                state=State.COMPLETED
            ))
        db.add_all(registrations)
        await db.commit()

        # EXPLAIN ANALYZEë¡œ ì‹¤í–‰ ê³„íš í™•ì¸
        stmt = select(ProductRegistration).where(
            ProductRegistration.state == State.PENDING
        )

        import time
        start = time.time()
        result = await db.execute(stmt)
        pending_items = result.scalars().all()
        elapsed = time.time() - start

        assert len(pending_items) == 500
        assert elapsed < 0.1  # 100ms ì´ë‚´ (Partial Index ì‚¬ìš© ì‹œ)

    @pytest.mark.asyncio
    async def test_composite_index_on_job_id_status(self, db):
        """(job_id, status) Composite Index íš¨ìœ¨ì„±"""
        # Test data setup...

        stmt = select(ProductRegistration).where(
            ProductRegistration.job_id == "job-123",
            ProductRegistration.state == State.COMPLETED
        )

        import time
        start = time.time()
        result = await db.execute(stmt)
        elapsed = time.time() - start

        assert elapsed < 0.05  # 50ms ì´ë‚´
```

---

## 6. End-to-End Testing Plan

### 6.1 Test Structure

```
tests/
â”œâ”€â”€ e2e/
â”‚   â”œâ”€â”€ test_single_product_registration.py   # ë‹¨ì¼ ìƒí’ˆ ë“±ë¡
â”‚   â”œâ”€â”€ test_batch_registration.py            # ëŒ€ëŸ‰ ë“±ë¡ (10ê°œ)
â”‚   â”œâ”€â”€ test_failure_scenarios.py             # ì‹¤íŒ¨ ì‹œë‚˜ë¦¬ì˜¤
â”‚   â””â”€â”€ test_manual_review_flow.py            # ìˆ˜ë™ ê²€í†  í”Œë¡œìš°
```

### 6.2 Single Product Registration (Happy Path)

**tests/e2e/test_single_product_registration.py:**

```python
import pytest
from httpx import AsyncClient
from app.main import app

class TestSingleProductRegistration:
    """ë‹¨ì¼ ìƒí’ˆ ë“±ë¡ E2E í…ŒìŠ¤íŠ¸ (ì „ì²´ í”Œë¡œìš°)"""

    @pytest.mark.e2e
    @pytest.mark.asyncio
    async def test_complete_registration_flow(self):
        """
        ì „ì²´ í”Œë¡œìš°:
        1. Job ìƒì„± (POST /jobs)
        2. ë„ë§¤ê¾¹ì—ì„œ ìƒí’ˆ ì¶”ì¶œ
        3. ë°ì´í„° ë³€í™˜
        4. ë„¤ì´ë²„ì— ë“±ë¡
        5. Job ì™„ë£Œ í™•ì¸
        """
        async with AsyncClient(app=app, base_url="http://test") as client:
            # Step 1: Create job
            response = await client.post(
                "/v1/jobs",
                json={
                    "type": "IMPORT",
                    "config": {
                        "source": "domeggook",
                        "filter": {"keyword": "í…ŒìŠ¤íŠ¸ìƒí’ˆ"},
                        "limit": 1,
                        "auto_register": True
                    }
                }
            )
            assert response.status_code == 201
            job_id = response.json()["data"]["job_id"]

            # Step 2: Wait for job completion (polling)
            import asyncio
            for _ in range(30):  # Max 30ì´ˆ ëŒ€ê¸°
                await asyncio.sleep(1)

                status_response = await client.get(f"/v1/jobs/{job_id}")
                status = status_response.json()["data"]["status"]

                if status in ["COMPLETED", "FAILED"]:
                    break

            # Step 3: Verify job completed successfully
            assert status == "COMPLETED"
            stats = status_response.json()["data"]["statistics"]
            assert stats["success_count"] == 1
            assert stats["failed_count"] == 0

            # Step 4: Verify product in database
            products_response = await client.get(
                f"/v1/jobs/{job_id}/products"
            )
            products = products_response.json()["data"]["items"]
            assert len(products) == 1
            assert products[0]["state"] == "COMPLETED"
            assert products[0]["naver_product_id"] is not None
```

---

### 6.3 Batch Registration Test

**tests/e2e/test_batch_registration.py:**

```python
import pytest
from httpx import AsyncClient
from app.main import app

class TestBatchRegistration:
    """ëŒ€ëŸ‰ ë“±ë¡ í…ŒìŠ¤íŠ¸ (10ê°œ)"""

    @pytest.mark.e2e
    @pytest.mark.asyncio
    async def test_batch_10_products_registration(self):
        """10ê°œ ìƒí’ˆ ëŒ€ëŸ‰ ë“±ë¡"""
        async with AsyncClient(app=app, base_url="http://test") as client:
            response = await client.post(
                "/v1/jobs",
                json={
                    "type": "IMPORT",
                    "config": {
                        "source": "domeggook",
                        "filter": {"category": "íŒ¨ì…˜ì˜ë¥˜"},
                        "limit": 10,
                        "auto_register": True
                    }
                }
            )

            job_id = response.json()["data"]["job_id"]

            # Wait for completion (max 5ë¶„)
            import asyncio
            for _ in range(300):
                await asyncio.sleep(1)

                status_response = await client.get(f"/v1/jobs/{job_id}")
                status = status_response.json()["data"]["status"]

                if status in ["COMPLETED", "FAILED"]:
                    break

            # Verify at least 80% success rate
            stats = status_response.json()["data"]["statistics"]
            success_rate = stats["success_count"] / stats["total_count"]
            assert success_rate >= 0.8, f"Success rate too low: {success_rate}"
```

---

### 6.4 Failure Scenarios Test

**tests/e2e/test_failure_scenarios.py:**

```python
import pytest
from httpx import AsyncClient
from app.main import app

class TestFailureScenarios:
    """ì‹¤íŒ¨ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸"""

    @pytest.mark.e2e
    @pytest.mark.asyncio
    async def test_invalid_category_mapping(self):
        """ì˜ëª»ëœ ì¹´í…Œê³ ë¦¬ ë§¤í•‘ ì²˜ë¦¬"""
        async with AsyncClient(app=app, base_url="http://test") as client:
            # Create product with unmapped category
            response = await client.post(
                "/v1/products",
                json={
                    "domeggook_item_id": "12345",
                    "category": "ì¡´ì¬í•˜ì§€ì•ŠëŠ”ì¹´í…Œê³ ë¦¬"
                }
            )

            assert response.status_code == 400
            assert "CATEGORY_NOT_MAPPED" in response.json()["error"]["code"]

    @pytest.mark.e2e
    @pytest.mark.asyncio
    async def test_forbidden_word_detection(self):
        """ê¸ˆì¹™ì–´ ê°ì§€ ì²˜ë¦¬"""
        async with AsyncClient(app=app, base_url="http://test") as client:
            response = await client.post(
                "/v1/products",
                json={
                    "name": "ë³‘ ì¹˜ë£Œì— íš¨ê³¼ì ì¸ ì œí’ˆ",
                    "price": 10000
                }
            )

            assert response.status_code == 400
            assert "FORBIDDEN_WORD" in response.json()["error"]["code"]

    @pytest.mark.e2e
    @pytest.mark.asyncio
    async def test_naver_api_429_retry_logic(self, monkeypatch):
        """ë„¤ì´ë²„ API 429 ì—ëŸ¬ ì‹œ ì¬ì‹œë„"""
        # Mock Naver API to return 429 on first 2 calls, then 200
        call_count = 0

        async def mock_register_product(self, data):
            nonlocal call_count
            call_count += 1
            if call_count <= 2:
                raise Exception("429 Too Many Requests")
            return {"success": True, "originProductNo": "12345"}

        monkeypatch.setattr(
            "app.connectors.naver_client.NaverClient.register_product",
            mock_register_product
        )

        async with AsyncClient(app=app, base_url="http://test") as client:
            response = await client.post("/v1/jobs", json={...})
            job_id = response.json()["data"]["job_id"]

            # Wait for retry and completion
            import asyncio
            await asyncio.sleep(10)  # Retry backoff time

            status_response = await client.get(f"/v1/jobs/{job_id}")

            # Should eventually succeed after 2 retries
            assert status_response.json()["data"]["status"] == "COMPLETED"
```

---

## 7. Performance & Load Testing Plan

### 7.1 Test Objectives

| Metric | Target | Test Tool |
|--------|--------|-----------|
| **Throughput** | 5,000 products/day | Locust |
| **Response Time** | P95 < 500ms | Locust |
| **Concurrent Workers** | 5 workers without rate limit violation | Custom script |
| **Rate Limiter Accuracy** | 0% violation (2 TPS exactly) | Redis monitoring |

### 7.2 Load Test Scenarios

**tests/performance/locustfile.py:**

```python
from locust import HttpUser, task, between
import random

class StoreBridgeUser(HttpUser):
    """Performance test user"""
    wait_time = between(1, 3)  # 1-3ì´ˆ ëŒ€ê¸°

    @task(3)
    def create_import_job(self):
        """ìƒí’ˆ ê°€ì ¸ì˜¤ê¸° Job ìƒì„± (ê°€ì¤‘ì¹˜ 3)"""
        self.client.post("/v1/jobs", json={
            "type": "IMPORT",
            "config": {
                "source": "domeggook",
                "filter": {"category": random.choice(["íŒ¨ì…˜ì˜ë¥˜", "ìƒí™œìš©í’ˆ"])},
                "limit": 10,
                "auto_register": True
            }
        })

    @task(2)
    def get_job_status(self):
        """Job ìƒíƒœ ì¡°íšŒ (ê°€ì¤‘ì¹˜ 2)"""
        job_id = "test-job-123"  # Assume exists
        self.client.get(f"/v1/jobs/{job_id}")

    @task(1)
    def get_manual_review_queue(self):
        """ìˆ˜ë™ ê²€í†  í ì¡°íšŒ (ê°€ì¤‘ì¹˜ 1)"""
        self.client.get("/v1/manual-review?page_size=20")
```

**ì‹¤í–‰ ëª…ë ¹:**
```bash
# 100ëª… ë™ì‹œ ì‚¬ìš©ì, ì´ˆë‹¹ 10ëª…ì”© ì¦ê°€, 5ë¶„ê°„ í…ŒìŠ¤íŠ¸
locust -f tests/performance/locustfile.py \
  --host https://staging.storebridge.com \
  --users 100 \
  --spawn-rate 10 \
  --run-time 5m \
  --html reports/load_test_report.html
```

---

### 7.3 Rate Limiter Stress Test

**tests/performance/test_rate_limiter_accuracy.py:**

```python
import pytest
import asyncio
import time
from app.services.rate_limiter import NaverRateLimiter

@pytest.mark.performance
@pytest.mark.asyncio
async def test_rate_limiter_accuracy_under_load():
    """5ê°œ ì›Œì»¤ê°€ ë™ì‹œì— ìš”ì²­ ì‹œ 2 TPS ì •í™•ë„"""
    limiter = NaverRateLimiter(max_tps=2)

    success_count = 0
    blocked_count = 0

    async def worker():
        """ê° ì›Œì»¤ê°€ 1ì´ˆì— 10ë²ˆ ìš”ì²­ ì‹œë„"""
        nonlocal success_count, blocked_count
        for _ in range(10):
            result = await limiter.acquire()
            if result:
                success_count += 1
            else:
                blocked_count += 1
            await asyncio.sleep(0.1)  # 0.1ì´ˆ ê°„ê²©

    # 5ê°œ ì›Œì»¤ ë™ì‹œ ì‹¤í–‰
    start = time.time()
    await asyncio.gather(*[worker() for _ in range(5)])
    elapsed = time.time() - start

    # ê²€ì¦: 1ì´ˆë‹¹ ì •í™•íˆ 2ê°œë§Œ ì„±ê³µí•´ì•¼ í•¨
    expected_success = int(elapsed) * 2
    tolerance = 1  # Â±1 í—ˆìš©

    assert abs(success_count - expected_success) <= tolerance, \
        f"Expected ~{expected_success}, got {success_count}"

    # Rate limit violation ì—†ìŒ
    assert success_count <= int(elapsed) * 2
```

**ì‹¤í–‰ ì‹œê°„:** 10ì´ˆ

---

### 7.4 Database Query Performance Test

**tests/performance/test_db_query_performance.py:**

```python
import pytest
import time
from sqlalchemy import select
from app.models import ProductRegistration, State

@pytest.mark.performance
@pytest.mark.asyncio
async def test_pending_queue_query_performance(db):
    """PENDING í ì¡°íšŒ ì„±ëŠ¥ (10,000ê°œ ì¤‘ 100ê°œ PENDING)"""
    # Create 10,000 records (100 PENDING, 9,900 COMPLETED)
    registrations = []
    for i in range(100):
        registrations.append(ProductRegistration(
            product_id=f"uuid-{i}",
            state=State.PENDING
        ))
    for i in range(100, 10000):
        registrations.append(ProductRegistration(
            product_id=f"uuid-{i}",
            state=State.COMPLETED
        ))
    db.add_all(registrations)
    await db.commit()

    # Query PENDING items
    stmt = select(ProductRegistration).where(
        ProductRegistration.state == State.PENDING
    ).limit(20)

    start = time.time()
    result = await db.execute(stmt)
    pending_items = result.scalars().all()
    elapsed = time.time() - start

    assert len(pending_items) == 20
    assert elapsed < 0.05, f"Query too slow: {elapsed}s (expected < 50ms)"
```

---

## 8. Test Data Management

### 8.1 Fixtures & Seed Data

**tests/fixtures/seed_data.py:**

```python
from app.models import Product, CategoryMapping

# Sample products from Domeggook
SAMPLE_PRODUCTS = [
    {
        "domeggook_item_id": "DG-001",
        "name": "ë©´ ë°˜íŒ” í‹°ì…”ì¸ ",
        "price": 15000,
        "category": "íŒ¨ì…˜ì˜ë¥˜",
        "images": [
            "https://domeggook.com/images/001_1.jpg",
            "https://domeggook.com/images/001_2.jpg"
        ],
        "options": ["ë¸”ë™-S", "ë¸”ë™-M", "í™”ì´íŠ¸-S", "í™”ì´íŠ¸-M"]
    },
    {
        "domeggook_item_id": "DG-002",
        "name": "ì²­ë°”ì§€",
        "price": 35000,
        "category": "íŒ¨ì…˜ì˜ë¥˜",
        "images": ["https://domeggook.com/images/002.jpg"],
        "options": ["28", "29", "30", "31", "32"]
    }
]

# Category mappings
CATEGORY_MAPPINGS = [
    {
        "domeggook_category": "íŒ¨ì…˜ì˜ë¥˜",
        "naver_leaf_category_id": "50000156",
        "required_attributes": {
            "ì œì¡°ì¼ì": {"type": "date", "required": True},
            "ì„¸íƒë°©ë²•": {"type": "string", "required": True}
        },
        "default_attributes": {
            "ì„¸íƒë°©ë²•": "ì¼ë°˜ì„¸íƒ"
        }
    },
    {
        "domeggook_category": "ìƒí™œìš©í’ˆ",
        "naver_leaf_category_id": "50000789",
        "required_attributes": {
            "ì œì¡°êµ­": {"type": "string", "required": True}
        }
    }
]

async def seed_test_data(db):
    """í…ŒìŠ¤íŠ¸ DBì— seed data ì‚½ì…"""
    for product_data in SAMPLE_PRODUCTS:
        product = Product(**product_data)
        db.add(product)

    for mapping_data in CATEGORY_MAPPINGS:
        mapping = CategoryMapping(**mapping_data)
        db.add(mapping)

    await db.commit()
```

---

### 8.2 VCR.py Cassette Management

**VCR Cassette ë””ë ‰í† ë¦¬ êµ¬ì¡°:**

```
tests/fixtures/vcr_cassettes/
â”œâ”€â”€ domeggook/
â”‚   â”œâ”€â”€ get_item_list.yaml
â”‚   â”œâ”€â”€ get_item_view.yaml
â”‚   â”œâ”€â”€ rate_limit_429.yaml
â”‚   â””â”€â”€ encoding_euc_kr.yaml
â”œâ”€â”€ naver/
â”‚   â”œâ”€â”€ upload_image.yaml
â”‚   â”œâ”€â”€ register_product.yaml
â”‚   â”œâ”€â”€ rate_limit_429.yaml
â”‚   â””â”€â”€ oauth_token.yaml
â””â”€â”€ README.md
```

**Cassette ì¬ìƒì„± (API ë³€ê²½ ì‹œ):**

```bash
# ê¸°ì¡´ cassette ì‚­ì œ í›„ ì¬ë…¹í™”
rm -rf tests/fixtures/vcr_cassettes/domeggook/get_item_list.yaml
VCR_RECORD_MODE=all pytest tests/integration/api/test_domeggook_integration.py::test_get_item_list
```

---

## 9. Mocking Strategy

### 9.1 Mocking Hierarchy

```
Real (Production)
    â†“
VCR.py (Integration Tests)
    â†“
AsyncMock (Unit Tests)
    â†“
Fake (In-memory, for E2E)
```

### 9.2 Fake Implementations

**tests/fakes/fake_naver_client.py:**

```python
from typing import Dict, Any

class FakeNaverClient:
    """ë„¤ì´ë²„ API Fake êµ¬í˜„ (í…ŒìŠ¤íŠ¸ìš©)"""

    def __init__(self):
        self.uploaded_images = []
        self.registered_products = []
        self.call_count = 0

    async def upload_image(self, image_data: bytes) -> Dict[str, Any]:
        """ì´ë¯¸ì§€ ì—…ë¡œë“œ (fake)"""
        self.call_count += 1
        fake_url = f"https://fake-cdn.naver.com/image_{self.call_count}.jpg"
        self.uploaded_images.append(fake_url)
        return {"success": True, "image_url": fake_url}

    async def register_product(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ìƒí’ˆ ë“±ë¡ (fake)"""
        self.call_count += 1
        fake_product_id = f"NAVER-{self.call_count:05d}"
        self.registered_products.append({
            "product_id": fake_product_id,
            "data": data
        })
        return {"success": True, "originProductNo": fake_product_id}

    def reset(self):
        """í…ŒìŠ¤íŠ¸ ê°„ ìƒíƒœ ì´ˆê¸°í™”"""
        self.uploaded_images.clear()
        self.registered_products.clear()
        self.call_count = 0
```

**ì‚¬ìš© ì˜ˆì‹œ:**

```python
@pytest.fixture
def fake_naver_client():
    client = FakeNaverClient()
    yield client
    client.reset()

def test_registration_uses_fake_client(fake_naver_client):
    """Fake client ì‚¬ìš© í…ŒìŠ¤íŠ¸"""
    result = await fake_naver_client.register_product({"name": "í…ŒìŠ¤íŠ¸"})

    assert result["success"] is True
    assert len(fake_naver_client.registered_products) == 1
```

---

## 10. CI/CD Integration

### 10.1 GitHub Actions Test Workflow

**ê¸°ì¡´ .github/workflows/ci-cd.ymlì— í†µí•©:**

```yaml
name: CI/CD Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

jobs:
  # ===== Unit & Integration Tests =====
  test:
    runs-on: ubuntu-latest

    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_USER: test
          POSTGRES_PASSWORD: test
          POSTGRES_DB: storebridge_test
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install -r requirements-dev.txt

      - name: Run Unit Tests
        run: |
          pytest tests/unit/ \
            --cov=app \
            --cov-report=xml \
            --cov-report=term \
            --junitxml=reports/unit-tests.xml

      - name: Run Integration Tests
        env:
          DATABASE_URL: postgresql://test:test@localhost:5432/storebridge_test
          REDIS_URL: redis://localhost:6379/1
          VCR_RECORD_MODE: none  # Use existing cassettes
        run: |
          pytest tests/integration/ \
            --junitxml=reports/integration-tests.xml

      - name: Upload Coverage to Codecov
        uses: codecov/codecov-action@v3
        with:
          files: ./coverage.xml
          fail_ci_if_error: true

      - name: Check Coverage Threshold
        run: |
          coverage report --fail-under=85

  # ===== E2E Tests (only on staging) =====
  e2e-test:
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/develop'
    needs: [test]

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Run E2E Tests against Staging
        env:
          API_BASE_URL: https://staging.storebridge.com
          DOMEGGOOK_API_KEY: ${{ secrets.DOMEGGOOK_SANDBOX_KEY }}
          NAVER_CLIENT_ID: ${{ secrets.NAVER_STAGING_CLIENT_ID }}
          NAVER_CLIENT_SECRET: ${{ secrets.NAVER_STAGING_SECRET }}
        run: |
          pytest tests/e2e/ \
            --junitxml=reports/e2e-tests.xml \
            -v

  # ===== Performance Tests (weekly schedule) =====
  performance-test:
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule'  # Cron trigger

    steps:
      - uses: actions/checkout@v4

      - name: Install Locust
        run: pip install locust

      - name: Run Load Test
        run: |
          locust -f tests/performance/locustfile.py \
            --host https://staging.storebridge.com \
            --users 100 \
            --spawn-rate 10 \
            --run-time 5m \
            --html reports/load_test_report.html \
            --headless

      - name: Upload Load Test Report
        uses: actions/upload-artifact@v3
        with:
          name: load-test-report
          path: reports/load_test_report.html
```

---

### 10.2 Pre-commit Hooks

**ì„¤ì¹˜:**

```bash
pip install pre-commit
pre-commit install
```

**.pre-commit-config.yaml:**

```yaml
repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.5.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-added-large-files

  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.1.6
    hooks:
      - id: ruff
        args: [--fix, --exit-non-zero-on-fix]

  - repo: https://github.com/pre-commit/mirrors-mypy
    rev: v1.7.0
    hooks:
      - id: mypy
        additional_dependencies: [types-all]

  - repo: local
    hooks:
      - id: pytest-unit
        name: Run Unit Tests
        entry: pytest tests/unit/ --maxfail=1
        language: system
        pass_filenames: false
        always_run: true
```

---

### 10.3 Test Reporting

**Pytest ì„¤ì •** (pytest.ini):

```ini
[pytest]
minversion = 7.0
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*

# Markers
markers =
    unit: Unit tests (fast, no external dependencies)
    integration: Integration tests (DB, Redis, APIs with VCR)
    e2e: End-to-end tests (full flow, slow)
    performance: Performance and load tests

# Coverage
addopts =
    --strict-markers
    --tb=short
    --cov=app
    --cov-report=html:reports/coverage
    --cov-report=term-missing
    --junitxml=reports/junit.xml

# Asyncio
asyncio_mode = auto

# Logging
log_cli = true
log_cli_level = INFO
```

**ì‹¤í–‰ ëª…ë ¹:**

```bash
# ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰
pytest

# Unit tests only (ë¹ ë¥¸ í”¼ë“œë°±)
pytest -m unit

# Integration tests only
pytest -m integration

# E2E tests only
pytest -m e2e

# Coverage report ìƒì„±
pytest --cov=app --cov-report=html
open reports/coverage/index.html
```

---

## 11. Test Execution Guide

### 11.1 ë¡œì»¬ ê°œë°œ í™˜ê²½

**Step 1: í™˜ê²½ ì„¤ì •**

```bash
# 1. Docker Composeë¡œ PostgreSQL, Redis ì‹œì‘
docker-compose up -d postgres redis

# 2. í…ŒìŠ¤íŠ¸ DB ìƒì„± ë° ë§ˆì´ê·¸ë ˆì´ì…˜
export DATABASE_URL=postgresql://test:test@localhost:5432/storebridge_test
alembic upgrade head

# 3. Seed data ì‚½ì…
python -m tests.fixtures.seed_data
```

**Step 2: í…ŒìŠ¤íŠ¸ ì‹¤í–‰**

```bash
# Unit tests (ê°€ì¥ ë¹ ë¦„ - 1ì´ˆ ì´ë‚´)
pytest -m unit -v

# Integration tests (VCR.py ì‚¬ìš© - 10ì´ˆ ì´ë‚´)
pytest -m integration -v

# íŠ¹ì • í…ŒìŠ¤íŠ¸ íŒŒì¼ë§Œ
pytest tests/unit/services/test_rate_limiter.py -v

# íŠ¹ì • í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë§Œ
pytest tests/unit/services/test_rate_limiter.py::TestNaverRateLimiter::test_acquire_success -v

# ì‹¤íŒ¨ ì‹œ ì¦‰ì‹œ ì¤‘ë‹¨ (--maxfail=1)
pytest -m unit --maxfail=1

# ë³‘ë ¬ ì‹¤í–‰ (pytest-xdist)
pytest -m unit -n auto  # CPU ì½”ì–´ ìˆ˜ë§Œí¼ ë³‘ë ¬
```

---

### 11.2 CI í™˜ê²½ (GitHub Actions)

**íŠ¸ë¦¬ê±°:**

1. **Push to main/develop**: Unit + Integration tests
2. **Pull Request**: Unit + Integration tests + Coverage check
3. **Push to develop**: E2E tests (staging í™˜ê²½)
4. **Weekly schedule**: Performance tests (Locust)

**í…ŒìŠ¤íŠ¸ ê²°ê³¼ í™•ì¸:**

```bash
# GitHub Actions íƒ­ì—ì„œ í™•ì¸
# - âœ… test job: Unit + Integration
# - âœ… e2e-test job: E2E (staging only)
# - ğŸ“Š Coverage report: Codecov badge
```

---

### 11.3 Staging í™˜ê²½

**Real API í…ŒìŠ¤íŠ¸ (VCR ì—†ì´):**

```bash
# Stagingì—ì„œ ì‹¤ì œ API í˜¸ì¶œ í…ŒìŠ¤íŠ¸
export ENVIRONMENT=staging
export VCR_RECORD_MODE=none  # VCR ë¹„í™œì„±í™”
export DOMEGGOOK_API_KEY=$DOMEGGOOK_SANDBOX_KEY
export NAVER_CLIENT_ID=$NAVER_STAGING_CLIENT_ID
export NAVER_CLIENT_SECRET=$NAVER_STAGING_SECRET

pytest tests/integration/api/ -v
```

---

## 12. Appendix: Sample Test Cases

### 12.1 Test Case Template

| ID | Test Name | Priority | Type | Precondition | Steps | Expected Result |
|----|-----------|----------|------|--------------|-------|-----------------|
| TC-001 | Rate Limiter blocks 3rd request | P0 | Unit | Rate limit = 2 TPS | 1. Call acquire() 3 times in 1 sec | 1st=True, 2nd=True, 3rd=False |
| TC-002 | Option Mapper parses 2D combo | P1 | Unit | Raw options = ["ë¸”ë™-S", "í™”ì´íŠ¸-M"] | 1. Call parse() | type=COMBINATION, dimensions=[ìƒ‰ìƒ, ì‚¬ì´ì¦ˆ] |
| TC-003 | Naver API upload image | P0 | Integration | Valid image data | 1. Call upload_image() | Returns CDN URL |
| TC-004 | Complete registration flow | P0 | E2E | Job created | 1. Create job<br>2. Wait for completion | Job status=COMPLETED |
| TC-005 | Rate limiter under load | P0 | Performance | 5 workers, 10 req/sec each | 1. Run 5 workers concurrently | Max 2 TPS enforced |

---

### 12.2 Test Coverage Report ì˜ˆì‹œ

**ëª©í‘œ Coverage (ë‹¨ìœ„ë³„):**

```
app/
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ rate_limiter.py          âœ… 95% (P0 - critical)
â”‚   â”œâ”€â”€ option_mapper.py         âœ… 90% (P1 - high)
â”‚   â””â”€â”€ image_processor.py       âœ… 85% (P1 - high)
â”œâ”€â”€ validators/
â”‚   â”œâ”€â”€ product_validator.py     âœ… 85%
â”‚   â”œâ”€â”€ category_validator.py    âœ… 80%
â”‚   â””â”€â”€ forbidden_word_validator.py âœ… 90%
â”œâ”€â”€ workflows/
â”‚   â””â”€â”€ registration_workflow.py âœ… 90% (State machine critical)
â”œâ”€â”€ connectors/
â”‚   â”œâ”€â”€ domeggook_client.py      âš ï¸  70% (ì™¸ë¶€ API ì˜ì¡´)
â”‚   â””â”€â”€ naver_client.py          âš ï¸  70% (ì™¸ë¶€ API ì˜ì¡´)
â””â”€â”€ transformers/
    â””â”€â”€ product_transformer.py   âœ… 85%

Overall Coverage: 85.3% âœ… (Target: 85%)
```

---

## 13. Version History

| Version | Date | Changes |
|---------|------|---------|
| 1.0.0 | 2025-10-16 | Initial test plan (Unit, Integration, E2E, Performance) |

---

**ë¬¸ì„œ ë - StoreBridge Test Plan v1.0.0**
